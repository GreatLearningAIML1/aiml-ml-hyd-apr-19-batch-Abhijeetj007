{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "name": "R8_Statistical_NLP_AbhijeetV2.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZH8W3fC7apaZ",
        "colab_type": "text"
      },
      "source": [
        "# Statistical NLP"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BpEw8ADuSKc1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "aff1f089-6932-47fc-910b-343c12a6296a"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "from sklearn.metrics import accuracy_score,classification_report,confusion_matrix\n",
        "import nltk\n",
        "import spacy\n",
        "from nltk.corpus import stopwords,wordnet\n",
        "from nltk.tokenize import word_tokenize, RegexpTokenizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn import metrics\n",
        "import string\n",
        "import re\n",
        "from sklearn.feature_extraction import text \n",
        "%matplotlib inline "
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/nltk/twitter/__init__.py:20: UserWarning: The twython library has not been installed. Some functionality from the twitter package will not be available.\n",
            "  warnings.warn(\"The twython library has not been installed. \"\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HPVyBvPDSZSl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81
        },
        "outputId": "2f2c3c9e-4586-490a-a2f9-efff7505966e"
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import keras\n",
        "from google.colab import drive"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H9DmVHFbTihx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "97a99ca1-a83d-482e-b289-6a306e380da8"
      },
      "source": [
        "import nltk\n",
        "nltk.download()   # Run this the first time after installing nltk package. It will open a GUI. Select all and hit Download."
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "NLTK Downloader\n",
            "---------------------------------------------------------------------------\n",
            "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
            "---------------------------------------------------------------------------\n",
            "Downloader> d\n",
            "\n",
            "Download which package (l=list; x=cancel)?\n",
            "  Identifier> all\n",
            "    Downloading collection 'all'\n",
            "       | \n",
            "       | Downloading package abc to /root/nltk_data...\n",
            "       |   Unzipping corpora/abc.zip.\n",
            "       | Downloading package alpino to /root/nltk_data...\n",
            "       |   Unzipping corpora/alpino.zip.\n",
            "       | Downloading package biocreative_ppi to /root/nltk_data...\n",
            "       |   Unzipping corpora/biocreative_ppi.zip.\n",
            "       | Downloading package brown to /root/nltk_data...\n",
            "       |   Unzipping corpora/brown.zip.\n",
            "       | Downloading package brown_tei to /root/nltk_data...\n",
            "       |   Unzipping corpora/brown_tei.zip.\n",
            "       | Downloading package cess_cat to /root/nltk_data...\n",
            "       |   Unzipping corpora/cess_cat.zip.\n",
            "       | Downloading package cess_esp to /root/nltk_data...\n",
            "       |   Unzipping corpora/cess_esp.zip.\n",
            "       | Downloading package chat80 to /root/nltk_data...\n",
            "       |   Unzipping corpora/chat80.zip.\n",
            "       | Downloading package city_database to /root/nltk_data...\n",
            "       |   Unzipping corpora/city_database.zip.\n",
            "       | Downloading package cmudict to /root/nltk_data...\n",
            "       |   Unzipping corpora/cmudict.zip.\n",
            "       | Downloading package comparative_sentences to\n",
            "       |     /root/nltk_data...\n",
            "       |   Unzipping corpora/comparative_sentences.zip.\n",
            "       | Downloading package comtrans to /root/nltk_data...\n",
            "       | Downloading package conll2000 to /root/nltk_data...\n",
            "       |   Unzipping corpora/conll2000.zip.\n",
            "       | Downloading package conll2002 to /root/nltk_data...\n",
            "       |   Unzipping corpora/conll2002.zip.\n",
            "       | Downloading package conll2007 to /root/nltk_data...\n",
            "       | Downloading package crubadan to /root/nltk_data...\n",
            "       |   Unzipping corpora/crubadan.zip.\n",
            "       | Downloading package dependency_treebank to /root/nltk_data...\n",
            "       |   Unzipping corpora/dependency_treebank.zip.\n",
            "       | Downloading package dolch to /root/nltk_data...\n",
            "       |   Unzipping corpora/dolch.zip.\n",
            "       | Downloading package europarl_raw to /root/nltk_data...\n",
            "       |   Unzipping corpora/europarl_raw.zip.\n",
            "       | Downloading package floresta to /root/nltk_data...\n",
            "       |   Unzipping corpora/floresta.zip.\n",
            "       | Downloading package framenet_v15 to /root/nltk_data...\n",
            "       |   Unzipping corpora/framenet_v15.zip.\n",
            "       | Downloading package framenet_v17 to /root/nltk_data...\n",
            "       |   Unzipping corpora/framenet_v17.zip.\n",
            "       | Downloading package gazetteers to /root/nltk_data...\n",
            "       |   Unzipping corpora/gazetteers.zip.\n",
            "       | Downloading package genesis to /root/nltk_data...\n",
            "       |   Unzipping corpora/genesis.zip.\n",
            "       | Downloading package gutenberg to /root/nltk_data...\n",
            "       |   Unzipping corpora/gutenberg.zip.\n",
            "       | Downloading package ieer to /root/nltk_data...\n",
            "       |   Unzipping corpora/ieer.zip.\n",
            "       | Downloading package inaugural to /root/nltk_data...\n",
            "       |   Unzipping corpora/inaugural.zip.\n",
            "       | Downloading package indian to /root/nltk_data...\n",
            "       |   Unzipping corpora/indian.zip.\n",
            "       | Downloading package jeita to /root/nltk_data...\n",
            "       | Downloading package kimmo to /root/nltk_data...\n",
            "       |   Unzipping corpora/kimmo.zip.\n",
            "       | Downloading package knbc to /root/nltk_data...\n",
            "       | Downloading package lin_thesaurus to /root/nltk_data...\n",
            "       |   Unzipping corpora/lin_thesaurus.zip.\n",
            "       | Downloading package mac_morpho to /root/nltk_data...\n",
            "       |   Unzipping corpora/mac_morpho.zip.\n",
            "       | Downloading package machado to /root/nltk_data...\n",
            "       | Downloading package masc_tagged to /root/nltk_data...\n",
            "       | Downloading package moses_sample to /root/nltk_data...\n",
            "       |   Unzipping models/moses_sample.zip.\n",
            "       | Downloading package movie_reviews to /root/nltk_data...\n",
            "       |   Unzipping corpora/movie_reviews.zip.\n",
            "       | Downloading package names to /root/nltk_data...\n",
            "       |   Unzipping corpora/names.zip.\n",
            "       | Downloading package nombank.1.0 to /root/nltk_data...\n",
            "       | Downloading package nps_chat to /root/nltk_data...\n",
            "       |   Unzipping corpora/nps_chat.zip.\n",
            "       | Downloading package omw to /root/nltk_data...\n",
            "       |   Unzipping corpora/omw.zip.\n",
            "       | Downloading package opinion_lexicon to /root/nltk_data...\n",
            "       |   Unzipping corpora/opinion_lexicon.zip.\n",
            "       | Downloading package paradigms to /root/nltk_data...\n",
            "       |   Unzipping corpora/paradigms.zip.\n",
            "       | Downloading package pil to /root/nltk_data...\n",
            "       |   Unzipping corpora/pil.zip.\n",
            "       | Downloading package pl196x to /root/nltk_data...\n",
            "       |   Unzipping corpora/pl196x.zip.\n",
            "       | Downloading package ppattach to /root/nltk_data...\n",
            "       |   Unzipping corpora/ppattach.zip.\n",
            "       | Downloading package problem_reports to /root/nltk_data...\n",
            "       |   Unzipping corpora/problem_reports.zip.\n",
            "       | Downloading package propbank to /root/nltk_data...\n",
            "       | Downloading package ptb to /root/nltk_data...\n",
            "       |   Unzipping corpora/ptb.zip.\n",
            "       | Downloading package product_reviews_1 to /root/nltk_data...\n",
            "       |   Unzipping corpora/product_reviews_1.zip.\n",
            "       | Downloading package product_reviews_2 to /root/nltk_data...\n",
            "       |   Unzipping corpora/product_reviews_2.zip.\n",
            "       | Downloading package pros_cons to /root/nltk_data...\n",
            "       |   Unzipping corpora/pros_cons.zip.\n",
            "       | Downloading package qc to /root/nltk_data...\n",
            "       |   Unzipping corpora/qc.zip.\n",
            "       | Downloading package reuters to /root/nltk_data...\n",
            "       | Downloading package rte to /root/nltk_data...\n",
            "       |   Unzipping corpora/rte.zip.\n",
            "       | Downloading package semcor to /root/nltk_data...\n",
            "       | Downloading package senseval to /root/nltk_data...\n",
            "       |   Unzipping corpora/senseval.zip.\n",
            "       | Downloading package sentiwordnet to /root/nltk_data...\n",
            "       |   Unzipping corpora/sentiwordnet.zip.\n",
            "       | Downloading package sentence_polarity to /root/nltk_data...\n",
            "       |   Unzipping corpora/sentence_polarity.zip.\n",
            "       | Downloading package shakespeare to /root/nltk_data...\n",
            "       |   Unzipping corpora/shakespeare.zip.\n",
            "       | Downloading package sinica_treebank to /root/nltk_data...\n",
            "       |   Unzipping corpora/sinica_treebank.zip.\n",
            "       | Downloading package smultron to /root/nltk_data...\n",
            "       |   Unzipping corpora/smultron.zip.\n",
            "       | Downloading package state_union to /root/nltk_data...\n",
            "       |   Unzipping corpora/state_union.zip.\n",
            "       | Downloading package stopwords to /root/nltk_data...\n",
            "       |   Unzipping corpora/stopwords.zip.\n",
            "       | Downloading package subjectivity to /root/nltk_data...\n",
            "       |   Unzipping corpora/subjectivity.zip.\n",
            "       | Downloading package swadesh to /root/nltk_data...\n",
            "       |   Unzipping corpora/swadesh.zip.\n",
            "       | Downloading package switchboard to /root/nltk_data...\n",
            "       |   Unzipping corpora/switchboard.zip.\n",
            "       | Downloading package timit to /root/nltk_data...\n",
            "       |   Unzipping corpora/timit.zip.\n",
            "       | Downloading package toolbox to /root/nltk_data...\n",
            "       |   Unzipping corpora/toolbox.zip.\n",
            "       | Downloading package treebank to /root/nltk_data...\n",
            "       |   Unzipping corpora/treebank.zip.\n",
            "       | Downloading package twitter_samples to /root/nltk_data...\n",
            "       |   Unzipping corpora/twitter_samples.zip.\n",
            "       | Downloading package udhr to /root/nltk_data...\n",
            "       |   Unzipping corpora/udhr.zip.\n",
            "       | Downloading package udhr2 to /root/nltk_data...\n",
            "       |   Unzipping corpora/udhr2.zip.\n",
            "       | Downloading package unicode_samples to /root/nltk_data...\n",
            "       |   Unzipping corpora/unicode_samples.zip.\n",
            "       | Downloading package universal_treebanks_v20 to\n",
            "       |     /root/nltk_data...\n",
            "       | Downloading package verbnet to /root/nltk_data...\n",
            "       |   Unzipping corpora/verbnet.zip.\n",
            "       | Downloading package verbnet3 to /root/nltk_data...\n",
            "       |   Unzipping corpora/verbnet3.zip.\n",
            "       | Downloading package webtext to /root/nltk_data...\n",
            "       |   Unzipping corpora/webtext.zip.\n",
            "       | Downloading package wordnet to /root/nltk_data...\n",
            "       |   Unzipping corpora/wordnet.zip.\n",
            "       | Downloading package wordnet_ic to /root/nltk_data...\n",
            "       |   Unzipping corpora/wordnet_ic.zip.\n",
            "       | Downloading package words to /root/nltk_data...\n",
            "       |   Unzipping corpora/words.zip.\n",
            "       | Downloading package ycoe to /root/nltk_data...\n",
            "       |   Unzipping corpora/ycoe.zip.\n",
            "       | Downloading package rslp to /root/nltk_data...\n",
            "       |   Unzipping stemmers/rslp.zip.\n",
            "       | Downloading package maxent_treebank_pos_tagger to\n",
            "       |     /root/nltk_data...\n",
            "       |   Unzipping taggers/maxent_treebank_pos_tagger.zip.\n",
            "       | Downloading package universal_tagset to /root/nltk_data...\n",
            "       |   Unzipping taggers/universal_tagset.zip.\n",
            "       | Downloading package maxent_ne_chunker to /root/nltk_data...\n",
            "       |   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "       | Downloading package punkt to /root/nltk_data...\n",
            "       |   Unzipping tokenizers/punkt.zip.\n",
            "       | Downloading package book_grammars to /root/nltk_data...\n",
            "       |   Unzipping grammars/book_grammars.zip.\n",
            "       | Downloading package sample_grammars to /root/nltk_data...\n",
            "       |   Unzipping grammars/sample_grammars.zip.\n",
            "       | Downloading package spanish_grammars to /root/nltk_data...\n",
            "       |   Unzipping grammars/spanish_grammars.zip.\n",
            "       | Downloading package basque_grammars to /root/nltk_data...\n",
            "       |   Unzipping grammars/basque_grammars.zip.\n",
            "       | Downloading package large_grammars to /root/nltk_data...\n",
            "       |   Unzipping grammars/large_grammars.zip.\n",
            "       | Downloading package tagsets to /root/nltk_data...\n",
            "       |   Unzipping help/tagsets.zip.\n",
            "       | Downloading package snowball_data to /root/nltk_data...\n",
            "       | Downloading package bllip_wsj_no_aux to /root/nltk_data...\n",
            "       |   Unzipping models/bllip_wsj_no_aux.zip.\n",
            "       | Downloading package word2vec_sample to /root/nltk_data...\n",
            "       |   Unzipping models/word2vec_sample.zip.\n",
            "       | Downloading package panlex_swadesh to /root/nltk_data...\n",
            "       | Downloading package mte_teip5 to /root/nltk_data...\n",
            "       |   Unzipping corpora/mte_teip5.zip.\n",
            "       | Downloading package averaged_perceptron_tagger to\n",
            "       |     /root/nltk_data...\n",
            "       |   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "       | Downloading package averaged_perceptron_tagger_ru to\n",
            "       |     /root/nltk_data...\n",
            "       |   Unzipping taggers/averaged_perceptron_tagger_ru.zip.\n",
            "       | Downloading package perluniprops to /root/nltk_data...\n",
            "       |   Unzipping misc/perluniprops.zip.\n",
            "       | Downloading package nonbreaking_prefixes to\n",
            "       |     /root/nltk_data...\n",
            "       |   Unzipping corpora/nonbreaking_prefixes.zip.\n",
            "       | Downloading package vader_lexicon to /root/nltk_data...\n",
            "       | Downloading package porter_test to /root/nltk_data...\n",
            "       |   Unzipping stemmers/porter_test.zip.\n",
            "       | Downloading package wmt15_eval to /root/nltk_data...\n",
            "       |   Unzipping models/wmt15_eval.zip.\n",
            "       | Downloading package mwa_ppdb to /root/nltk_data...\n",
            "       |   Unzipping misc/mwa_ppdb.zip.\n",
            "       | \n",
            "     Done downloading collection all\n",
            "\n",
            "---------------------------------------------------------------------------\n",
            "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
            "---------------------------------------------------------------------------\n",
            "Downloader> 1\n",
            "Command '1' unrecognized\n",
            "\n",
            "---------------------------------------------------------------------------\n",
            "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
            "---------------------------------------------------------------------------\n",
            "Downloader> q\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NwGN4DlGSTQk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        },
        "outputId": "6ed708f7-a72b-4ffa-a2d9-f00bdbc442d3"
      },
      "source": [
        "drive.mount('/content/drive')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B5w727xNSyUs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "os.chdir('/content/drive/My Drive/AIML/R8/Project')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zu8F3dwaS0PV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data = pd.read_csv(r'/content/drive/My Drive/AIML/R8/Project/blogtext.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5gVHarq-SKdE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "234cdc2d-f416-425d-e6e9-19fe78368c7a"
      },
      "source": [
        "data.columns\n"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['id', 'gender', 'age', 'topic', 'sign', 'date', 'text'], dtype='object')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4q_mkEU3SKdK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "106e9829-4d6a-4d86-99c6-4150432bb20a"
      },
      "source": [
        "data.shape"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(681284, 7)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AWq-gsrYSKdO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 230
        },
        "outputId": "4b0410c5-d631-462b-8762-7e3f4345a4a5"
      },
      "source": [
        "data['text']"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0                    Info has been found (+/- 100 pages,...\n",
              "1                    These are the team members:   Drewe...\n",
              "2                    In het kader van kernfusie op aarde...\n",
              "3                          testing!!!  testing!!!          \n",
              "4                      Thanks to Yahoo!'s Toolbar I can ...\n",
              "                                ...                        \n",
              "681279           Dear Susan,  I could write some really ...\n",
              "681280           Dear Susan,  'I have the second yeast i...\n",
              "681281           Dear Susan,  Your 'boyfriend' is fuckin...\n",
              "681282           Dear Susan:    Just to clarify, I am as...\n",
              "681283           Hey everybody...and Susan,  You might a...\n",
              "Name: text, Length: 681284, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yGJnkQ8HSKdT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "new_data = data.head(9000)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "07URRMAYSKdX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 123
        },
        "outputId": "9f0e065d-6475-4182-afbe-aff194f8102f"
      },
      "source": [
        "new_data['label'] = new_data[new_data.columns[1:5]].apply(lambda x: (','.join(x.dropna().astype(str))),axis=1)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fnCxUXhlSKde",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 406
        },
        "outputId": "f4d7e0d6-69c3-4b97-e927-241cc8c25b8f"
      },
      "source": [
        "new_data"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>gender</th>\n",
              "      <th>age</th>\n",
              "      <th>topic</th>\n",
              "      <th>sign</th>\n",
              "      <th>date</th>\n",
              "      <th>text</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2059027</td>\n",
              "      <td>male</td>\n",
              "      <td>15</td>\n",
              "      <td>Student</td>\n",
              "      <td>Leo</td>\n",
              "      <td>14,May,2004</td>\n",
              "      <td>Info has been found (+/- 100 pages,...</td>\n",
              "      <td>male,15,Student,Leo</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2059027</td>\n",
              "      <td>male</td>\n",
              "      <td>15</td>\n",
              "      <td>Student</td>\n",
              "      <td>Leo</td>\n",
              "      <td>13,May,2004</td>\n",
              "      <td>These are the team members:   Drewe...</td>\n",
              "      <td>male,15,Student,Leo</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2059027</td>\n",
              "      <td>male</td>\n",
              "      <td>15</td>\n",
              "      <td>Student</td>\n",
              "      <td>Leo</td>\n",
              "      <td>12,May,2004</td>\n",
              "      <td>In het kader van kernfusie op aarde...</td>\n",
              "      <td>male,15,Student,Leo</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2059027</td>\n",
              "      <td>male</td>\n",
              "      <td>15</td>\n",
              "      <td>Student</td>\n",
              "      <td>Leo</td>\n",
              "      <td>12,May,2004</td>\n",
              "      <td>testing!!!  testing!!!</td>\n",
              "      <td>male,15,Student,Leo</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>3581210</td>\n",
              "      <td>male</td>\n",
              "      <td>33</td>\n",
              "      <td>InvestmentBanking</td>\n",
              "      <td>Aquarius</td>\n",
              "      <td>11,June,2004</td>\n",
              "      <td>Thanks to Yahoo!'s Toolbar I can ...</td>\n",
              "      <td>male,33,InvestmentBanking,Aquarius</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8995</th>\n",
              "      <td>3477296</td>\n",
              "      <td>male</td>\n",
              "      <td>15</td>\n",
              "      <td>Student</td>\n",
              "      <td>Virgo</td>\n",
              "      <td>17,July,2004</td>\n",
              "      <td>wah! how cold was it today? *...</td>\n",
              "      <td>male,15,Student,Virgo</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8996</th>\n",
              "      <td>3477296</td>\n",
              "      <td>male</td>\n",
              "      <td>15</td>\n",
              "      <td>Student</td>\n",
              "      <td>Virgo</td>\n",
              "      <td>17,July,2004</td>\n",
              "      <td>Could someone give me some su...</td>\n",
              "      <td>male,15,Student,Virgo</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8997</th>\n",
              "      <td>3477296</td>\n",
              "      <td>male</td>\n",
              "      <td>15</td>\n",
              "      <td>Student</td>\n",
              "      <td>Virgo</td>\n",
              "      <td>17,July,2004</td>\n",
              "      <td>I am blogging here tonight on...</td>\n",
              "      <td>male,15,Student,Virgo</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8998</th>\n",
              "      <td>3477296</td>\n",
              "      <td>male</td>\n",
              "      <td>15</td>\n",
              "      <td>Student</td>\n",
              "      <td>Virgo</td>\n",
              "      <td>16,July,2004</td>\n",
              "      <td>'It isn't pollution that's ha...</td>\n",
              "      <td>male,15,Student,Virgo</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8999</th>\n",
              "      <td>3477296</td>\n",
              "      <td>male</td>\n",
              "      <td>15</td>\n",
              "      <td>Student</td>\n",
              "      <td>Virgo</td>\n",
              "      <td>16,July,2004</td>\n",
              "      <td>Seems like everyone these day...</td>\n",
              "      <td>male,15,Student,Virgo</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>9000 rows × 8 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "           id  ...                               label\n",
              "0     2059027  ...                 male,15,Student,Leo\n",
              "1     2059027  ...                 male,15,Student,Leo\n",
              "2     2059027  ...                 male,15,Student,Leo\n",
              "3     2059027  ...                 male,15,Student,Leo\n",
              "4     3581210  ...  male,33,InvestmentBanking,Aquarius\n",
              "...       ...  ...                                 ...\n",
              "8995  3477296  ...               male,15,Student,Virgo\n",
              "8996  3477296  ...               male,15,Student,Virgo\n",
              "8997  3477296  ...               male,15,Student,Virgo\n",
              "8998  3477296  ...               male,15,Student,Virgo\n",
              "8999  3477296  ...               male,15,Student,Virgo\n",
              "\n",
              "[9000 rows x 8 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tulgKORiSKdi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "new_data = new_data.drop(columns=['gender','age','topic','sign','id','date'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SYldPHDVSKdl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 406
        },
        "outputId": "e2c86ebc-77df-4d5f-bdc0-a0cedc671306"
      },
      "source": [
        "new_data"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Info has been found (+/- 100 pages,...</td>\n",
              "      <td>male,15,Student,Leo</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>These are the team members:   Drewe...</td>\n",
              "      <td>male,15,Student,Leo</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>In het kader van kernfusie op aarde...</td>\n",
              "      <td>male,15,Student,Leo</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>testing!!!  testing!!!</td>\n",
              "      <td>male,15,Student,Leo</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Thanks to Yahoo!'s Toolbar I can ...</td>\n",
              "      <td>male,33,InvestmentBanking,Aquarius</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8995</th>\n",
              "      <td>wah! how cold was it today? *...</td>\n",
              "      <td>male,15,Student,Virgo</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8996</th>\n",
              "      <td>Could someone give me some su...</td>\n",
              "      <td>male,15,Student,Virgo</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8997</th>\n",
              "      <td>I am blogging here tonight on...</td>\n",
              "      <td>male,15,Student,Virgo</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8998</th>\n",
              "      <td>'It isn't pollution that's ha...</td>\n",
              "      <td>male,15,Student,Virgo</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8999</th>\n",
              "      <td>Seems like everyone these day...</td>\n",
              "      <td>male,15,Student,Virgo</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>9000 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                   text                               label\n",
              "0                Info has been found (+/- 100 pages,...                 male,15,Student,Leo\n",
              "1                These are the team members:   Drewe...                 male,15,Student,Leo\n",
              "2                In het kader van kernfusie op aarde...                 male,15,Student,Leo\n",
              "3                      testing!!!  testing!!!                           male,15,Student,Leo\n",
              "4                  Thanks to Yahoo!'s Toolbar I can ...  male,33,InvestmentBanking,Aquarius\n",
              "...                                                 ...                                 ...\n",
              "8995                   wah! how cold was it today? *...               male,15,Student,Virgo\n",
              "8996                   Could someone give me some su...               male,15,Student,Virgo\n",
              "8997                   I am blogging here tonight on...               male,15,Student,Virgo\n",
              "8998                   'It isn't pollution that's ha...               male,15,Student,Virgo\n",
              "8999                   Seems like everyone these day...               male,15,Student,Virgo\n",
              "\n",
              "[9000 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QYK927qRSKdo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d40c0fb2-2a01-4f2a-c3d1-8f331c8bb502"
      },
      "source": [
        "print(len(new_data.text))"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "9000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1soq4zBISKdt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a241aec0-4138-4382-aa38-0d4ccd3bb0ff"
      },
      "source": [
        "new_data.text[0]"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'           Info has been found (+/- 100 pages, and 4.5 MB of .pdf files) Now i have to wait untill our team leader has processed it and learns html.         '"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VsOXiUP6SKdx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "nlp = spacy.load('en_core_web_sm')\n",
        "l1 = ('btw','zza','zzzexy','zzzzz','youuuuu')\n",
        "nlp.Defaults.stop_words.add(l1)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E6zIeIvRSKd0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#for i in range(len(new_data.text)):\n",
        " #   new_data.text[i] = new_data.text[i].lower()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r30ZQiBTSKd4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 230
        },
        "outputId": "835493bd-438c-4dae-e634-a87adc04ec01"
      },
      "source": [
        "new_data.text"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0                  Info has been found (+/- 100 pages,...\n",
              "1                  These are the team members:   Drewe...\n",
              "2                  In het kader van kernfusie op aarde...\n",
              "3                        testing!!!  testing!!!          \n",
              "4                    Thanks to Yahoo!'s Toolbar I can ...\n",
              "                              ...                        \n",
              "8995                     wah! how cold was it today? *...\n",
              "8996                     Could someone give me some su...\n",
              "8997                     I am blogging here tonight on...\n",
              "8998                     'It isn't pollution that's ha...\n",
              "8999                     Seems like everyone these day...\n",
              "Name: text, Length: 9000, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gQU33OGzSKd8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "0046e268-e62a-4ab5-c715-e3a049cac6f5"
      },
      "source": [
        "new_data.dtypes"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "text     object\n",
              "label    object\n",
              "dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MUPqnShlSKeB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "d620d5bf-3533-468b-95b1-47fb9a3e3390"
      },
      "source": [
        "for i in range(len(new_data.text)):\n",
        "    tokenizer = RegexpTokenizer(r'\\w+')\n",
        "    new_data.text[i] = new_data.text[i].lower()\n",
        "    word_tokens = tokenizer.tokenize(new_data.text[i])\n",
        "    filtered_sentence = [w for w in word_tokens if not w in stopwords.words('english')] \n",
        "    filtered_sentence = [] \n",
        "    for w in word_tokens: \n",
        "        if w not in (nlp.Defaults.stop_words or string.punctuation):\n",
        "            #if not w.isalpha():\n",
        "            filtered_sentence.append(re.sub(r\"[^a-zA-Z0-9]+\", ' ',w ))\n",
        "   \n",
        "    new_data.text[i] = \" \".join(filtered_sentence)\n",
        "#print(word_tokens) \n",
        "#    print(new_data.text_new[i])\n",
        "%time"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 2 µs, sys: 1 µs, total: 3 µs\n",
            "Wall time: 5.48 µs\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "293N5OM4SKeE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 230
        },
        "outputId": "90124064-552c-469e-929f-8f11ce26b652"
      },
      "source": [
        "new_data.text"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0       info found 100 pages 4 5 mb pdf files wait unt...\n",
              "1       team members drewes van der laag urllink mail ...\n",
              "2       het kader van kernfusie op aarde maak je eigen...\n",
              "3                                         testing testing\n",
              "4       thanks yahoo s toolbar capture urls popups mea...\n",
              "                              ...                        \n",
              "8995    wah cold today brrr wake 7 worth grins said sl...\n",
              "8996    suggestions music dls im open asian music look...\n",
              "8997    blogging tonight request andy know blog tonigh...\n",
              "8998    isn t pollution s harming environment s impuri...\n",
              "8999    like days turning religion times peril terror ...\n",
              "Name: text, Length: 9000, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g4f7RZ7hSKeI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X = new_data.text\n",
        "Y = new_data.label"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vjPJfJ1USKeM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# split into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, Y, random_state=524)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eXtZMB3oSKeT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "7caacc51-f7a3-4547-bd75-7038765deb7f"
      },
      "source": [
        "# define a function that accepts a vectorizer and calculates the accuracy\n",
        "def tokenize_test(vect):\n",
        "    X_train_dtm = vect.fit_transform(X_train)\n",
        "    print('Features: ', X_train_dtm.shape[1])\n",
        "    X_test_dtm = vect.transform(X_test)\n",
        "    nb = MultinomialNB()\n",
        "    nb.fit(X_train_dtm, y_train)\n",
        "    #feature = nb.feature_count_\n",
        "    #print(\"NB feature shape\", feature)\n",
        "   # print(nb.feature_count_.shape)\n",
        "    y_train_pred = nb.predict(X_train_dtm)\n",
        "    y_pred_class = nb.predict(X_test_dtm)\n",
        "    print('Train Accuracy for NB : ', metrics.accuracy_score(y_train,y_train_pred))\n",
        "    print('Test Accuracy for NB: ', metrics.accuracy_score(y_test, y_pred_class))\n",
        "    logreg = LogisticRegression(C=1e9)\n",
        "    logreg.fit(X_train_dtm, y_train)\n",
        "    y_pred_class_LR = logreg.predict(X_test_dtm)\n",
        "#print(metrics.accuracy_score(y_test, y_pred_class))\n",
        "    y_train_LR = logreg.predict(X_train_dtm)\n",
        "    print('Train Accuracy for LR: ',metrics.accuracy_score(y_train, y_train_LR))\n",
        "    print('Test Accuracy for LR: ',metrics.accuracy_score(y_test, y_pred_class_LR))\n",
        "%time"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 4 µs, sys: 1 µs, total: 5 µs\n",
            "Wall time: 7.15 µs\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zCok4LcFSKeY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 232
        },
        "outputId": "61b57320-fba8-48b5-cc87-02f33d032ae2"
      },
      "source": [
        "# include 1-grams and 2-grams\n",
        "vect = CountVectorizer(ngram_range=(1, 2))\n",
        "tokenize_test(vect)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Features:  418363\n",
            "Train Accuracy for NB :  0.7691851851851852\n",
            "Test Accuracy for NB:  0.3671111111111111\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
            "  FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
            "  \"this warning.\", FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train Accuracy for LR:  0.9948148148148148\n",
            "Test Accuracy for LR:  0.5008888888888889\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t-vlS86fSKef",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "9028b57d-8408-44cc-83b4-95c5a075989c"
      },
      "source": [
        "X_train_dtm = vect.fit_transform(X_train)\n",
        "print('Features: ', X_train_dtm.shape[1])\n",
        "X_test_dtm = vect.transform(X_test)\n",
        "nb = MultinomialNB()\n",
        "nb.fit(X_train_dtm, y_train)\n",
        "    #feature = nb.feature_count_\n",
        "    #print(\"NB feature shape\", feature)\n",
        "   # print(nb.feature_count_.shape)\n",
        "y_train_pred = nb.predict(X_train_dtm)\n",
        "y_pred_class = nb.predict(X_test_dtm)\n",
        "print('Train Accuracy for NB : ', metrics.accuracy_score(y_train,y_train_pred))\n",
        "print('Test Accuracy for NB: ', metrics.accuracy_score(y_test, y_pred_class))"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Features:  418363\n",
            "Train Accuracy for NB :  0.7691851851851852\n",
            "Test Accuracy for NB:  0.3671111111111111\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e5HXPdRlSKek",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "02759586-53cd-4260-c077-32b4f91820b1"
      },
      "source": [
        "# features names\n",
        "feature_names = vect.get_feature_names()\n",
        "print(feature_names[50:500])"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['00 newton', '00 night', '00 nospamyahoo', '00 ok', '00 ouch', '00 plus', '00 pm', '00 presentations', '00 rarely', '00 ravenwood', '00 recently', '00 said', '00 saw', '00 sec', '00 showing', '00 speeding', '00 store', '00 sumber', '00 supposed', '00 taken', '00 tcr1', '00 teh', '00 time', '00 tired', '00 today', '00 tomorrow', '00 truth', '00 unit', '00 usher', '00 ve', '00 week', '00 woke', '00 yahoo', '00 yes', '000', '000 00', '000 000', '000 100', '000 1998', '000 36', '000 acre', '000 active', '000 address', '000 albums', '000 amish', '000 animals', '000 automobile', '000 bad', '000 btu', '000 cds', '000 checks', '000 companion', '000 compared', '000 copies', '000 cops', '000 country', '000 dangerous', '000 dollar', '000 dollars', '000 drop', '000 end', '000 english', '000 excited', '000 fact', '000 families', '000 far', '000 feet', '000 ferrari', '000 fight', '000 figure', '000 financial', '000 fix', '000 flights', '000 fucking', '000 ground', '000 guess', '000 head', '000 hearing', '000 households', '000 human', '000 insane', '000 insurance', '000 iraqi', '000 japanese', '000 jesus', '000 jobs', '000 kuwait', '000 lakes', '000 layers', '000 loan', '000 makes', '000 miles', '000 month', '000 new', '000 nutjobs', '000 options', '000 order', '000 pages', '000 payment', '000 people', '000 piece', '000 pledge', '000 pound', '000 pounds', '000 protestors', '000 relief', '000 renovation', '000 rich', '000 said', '000 sentenced', '000 shit', '000 signatures', '000 slaughtered', '000 soldiers', '000 square', '000 strangers', '000 subject', '000 texas', '000 times', '000 troops', '000 united', '000 ve', '000 visitors', '000 votes', '000 wants', '000 won', '000 words', '000 worth', '000 year', '000 years', '000 yes', '0000', '0000 blinking', '000001', '000001 maybe', '000001 zbaras', '000058', '000058 html', '000miles', '000miles away', '000th', '000th time', '001', '001 dollars', '001 jillian', '002', '002 middle', '003', '003 love', '004', '004 nickname', '005', '005 gender', '006', '006 age', '007', '007 birthday', '007 game', '007 jersey', '008', '008 height', '009', '009 hair', '00am', '00am early', '00am going', '00am know', '00am ride', '00am somebody', '00am today', '00pm', '00pm check', '00pm darn', '00pm especially', '00pm figured', '00pm hahahhahaha', '00pm played', '00pm till', '01', '01 08', '01 2003', '01 22', '01 informs', '01 known', '01 role', '01 televive', '01 underworld', '010', '010 eye', '0100', '0100 hrs', '010203', '010203 hehehe', '011', '011 race', '012', '012 glasses', '01234', '01234 time', '0128', '0128 euro', '013', '013 braces', '014', '014 949', '014 hair', '015', '015 born', '016', '016 current', '017', '017 zodiac', '018', '018 languages', '019', '019 nationality', '02', '02 00', '02 18', '02 2002', '02 23', '02 25', '02 56', '02 added', '02 argue', '02 deadbeat', '02 faced', '02 hope', '02 impression', '02 like', '02 ll', '02 lott', '02 personal', '02 pet', '02 pm', '02 republicans', '02 statement', '02 urllink', '02 wilding', '020', '020 bad', '021', '021 piercings', '022', '022 piercings', '023', '023 tattoos', '024', '024 tattoos', '025', '025 today', '0250', '0250 hentoff', '026', '026 time', '027', '027 ready', '028', '028 mother', '029', '029 father', '03', '03 03', '03 11', '03 15', '03 16', '03 22', '03 31', '03 41', '03 afternoon', '03 calvin', '03 chewbacca', '03 confused', '03 deadbeat', '03 deal', '03 fiesta', '03 focus', '03 god', '03 harmony', '03 hate', '03 knows', '03 life', '03 love', '03 moz', '03 pass', '03 pm', '03 sarah', '03 sigh', '03 spending', '03 state', '03 won', '03 working', '03 world', '03 wouldn', '030', '030 step', '031', '031 brother', '032', '032 sister', '0327554', '033', '033 favorite', '034', '034 favorite', '0346', '0346 otis', '035', '035 favorite', '036', '036 worst', '037', '037 best', '038', '038 parents', '039', '039 family', '04', '04 04', '04 10', '04 16', '04 19', '04 added', '04 apocalypse', '04 asses', '04 better', '04 brainer', '04 bush', '04 capitalize', '04 cia', '04 compassionate', '04 couldnt', '04 day', '04 don', '04 economy', '04 email', '04 going', '04 gooder', '04 heart', '04 leave', '04 liked', '04 making', '04 ntu', '04 pineapple', '04 president', '04 putting', '04 revolutionary', '04 season', '04 seeing', '04 state', '04 thank', '04 thanks', '04 ticket', '04 time', '04 told', '04 trustworthy', '04 truth', '04 vote', '04 wars', '040', '040 pets', '0400', '0400 starin', '041', '041 names', '042', '042 kind', '043', '043 school', '0431', '0431 travsd', '044', '044 drop', '045', '045 current', '046', '046 favorite', '047', '047 favorite', '048', '048 favorite', '049', '049 favorite', '05', '05 04', '05 06', '05 07', '05 11', '05 13', '05 16', '05 22', '05 35', '05 44', '05 cried', '05 drugwar', '05 gives', '05 greeted', '05 jul', '05 little', '05 makes', '05 quickly', '05 sweet', '05 thursday', '05 trouble', '05 watched', '050', '050 favorite', '0505', '0505 189', '051', '051 favorite', '052', '052 buy', '053', '053 play', '054', '054 extracurricular', '055', '055 popular', '056', '056 favorite', '057', '057 favorite', '058', '058 favorite', '059', '059 favorite', '05pm', '05pm day', '06', '06 03', '06 04', '06 05', '06 07', '06 09', '06 10', '06 11', '06 12', '06 19', '06 case', '06 cloud', '06 hard', '06 pm', '06 pst', '06 slave', '06 sure', '06 type', '060', '060 humiliating', '0600']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sp_-sT3ESKeo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "outputId": "fc913f3b-46aa-4b8d-a5fa-c9edc51ab180"
      },
      "source": [
        "y_train.head(4)"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2122    male,35,Technology,Aries\n",
              "5676     female,27,indUnk,Taurus\n",
              "7396       male,36,Fashion,Aries\n",
              "1668    male,35,Technology,Aries\n",
              "Name: label, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dpqynUC_SKeu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c6616835-5104-4373-eb5c-d94507623a09"
      },
      "source": [
        "len(y_train)"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "6750"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sFzQCqKQSKe0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "d = {}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G5rHZSWMSKe4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "d = y_train.apply(lambda x : pd.value_counts(x.split(\",\"))).sum(axis = 0).to_dict()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-awb39_oSKe8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "07a31270-6bd2-476f-cbdc-799f2aa86485"
      },
      "source": [
        "d"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'13': 29.0,\n",
              " '14': 133.0,\n",
              " '15': 416.0,\n",
              " '16': 126.0,\n",
              " '17': 745.0,\n",
              " '23': 156.0,\n",
              " '24': 346.0,\n",
              " '25': 270.0,\n",
              " '26': 143.0,\n",
              " '27': 665.0,\n",
              " '33': 102.0,\n",
              " '34': 414.0,\n",
              " '35': 1753.0,\n",
              " '36': 1298.0,\n",
              " '37': 13.0,\n",
              " '38': 37.0,\n",
              " '39': 60.0,\n",
              " '40': 1.0,\n",
              " '41': 15.0,\n",
              " '42': 9.0,\n",
              " '43': 4.0,\n",
              " '44': 2.0,\n",
              " '45': 10.0,\n",
              " '46': 3.0,\n",
              " 'Accounting': 4.0,\n",
              " 'Aquarius': 301.0,\n",
              " 'Aries': 3155.0,\n",
              " 'Arts': 26.0,\n",
              " 'Automotive': 7.0,\n",
              " 'Banking': 14.0,\n",
              " 'BusinessServices': 62.0,\n",
              " 'Cancer': 230.0,\n",
              " 'Capricorn': 101.0,\n",
              " 'Communications-Media': 76.0,\n",
              " 'Consulting': 10.0,\n",
              " 'Education': 184.0,\n",
              " 'Engineering': 102.0,\n",
              " 'Fashion': 1230.0,\n",
              " 'Gemini': 107.0,\n",
              " 'Internet': 89.0,\n",
              " 'InvestmentBanking': 54.0,\n",
              " 'Law': 6.0,\n",
              " 'Leo': 163.0,\n",
              " 'Libra': 372.0,\n",
              " 'Marketing': 9.0,\n",
              " 'Museums-Libraries': 2.0,\n",
              " 'Non-Profit': 57.0,\n",
              " 'Pisces': 288.0,\n",
              " 'Publishing': 2.0,\n",
              " 'Religion': 8.0,\n",
              " 'Sagittarius': 664.0,\n",
              " 'Science': 45.0,\n",
              " 'Scorpio': 672.0,\n",
              " 'Sports-Recreation': 53.0,\n",
              " 'Student': 689.0,\n",
              " 'Taurus': 588.0,\n",
              " 'Technology': 1970.0,\n",
              " 'Virgo': 109.0,\n",
              " 'female': 2590.0,\n",
              " 'indUnk': 2051.0,\n",
              " 'male': 4160.0}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q3_91QxaSKfB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.preprocessing import MultiLabelBinarizer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IZlJcWMtSKfH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mlb = MultiLabelBinarizer()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K8AjsbKqSKfP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_train_mlb = mlb.fit_transform(y_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0w5m2msbSKfa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_test_mlb = mlb.transform(y_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d0j5NkRJSKfm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "from sklearn.linear_model import LogisticRegression"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KUknZZEdSKfs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "LR = LogisticRegression(solver = 'lbfgs',random_state= 111)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t8581PRISKfv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "outputId": "e5128272-58c5-4f98-d25e-5bb25468133f"
      },
      "source": [
        "LR"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
              "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
              "                   multi_class='warn', n_jobs=None, penalty='l2',\n",
              "                   random_state=111, solver='lbfgs', tol=0.0001, verbose=0,\n",
              "                   warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xsi6vuwHSKfy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "clf = OneVsRestClassifier(LR)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OgcF8ugVSKf2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "names = vect.get_feature_names()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GUJPPMeVSKf4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "outputId": "47cc7c69-46a4-4b58-e068-6e58f3a0bc4b"
      },
      "source": [
        "y_train_mlb"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1, 0, 0, ..., 0, 0, 1],\n",
              "       [1, 0, 0, ..., 0, 0, 0],\n",
              "       [1, 0, 0, ..., 0, 0, 0],\n",
              "       ...,\n",
              "       [1, 0, 0, ..., 0, 0, 1],\n",
              "       [1, 0, 0, ..., 0, 0, 0],\n",
              "       [1, 0, 0, ..., 0, 0, 0]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3RI8UXIxSKf6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 941
        },
        "outputId": "b49cbe0d-043b-4e2e-81c0-15037827bd35"
      },
      "source": [
        "clf.fit(X_train_dtm,y_train_mlb)\n",
        "%time"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/multiclass.py:76: UserWarning: Label 0 is present in all training examples.\n",
            "  str(classes[c]))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
            "  \"of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
            "  \"of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
            "  \"of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
            "  \"of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
            "  \"of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
            "  \"of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
            "  \"of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
            "  \"of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/multiclass.py:76: UserWarning: Label 28 is present in all training examples.\n",
            "  str(classes[c]))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
            "  \"of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
            "  \"of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/multiclass.py:76: UserWarning: Label 32 is present in all training examples.\n",
            "  str(classes[c]))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
            "  \"of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
            "  \"of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
            "  \"of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
            "  \"of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/multiclass.py:76: UserWarning: Label 38 is present in all training examples.\n",
            "  str(classes[c]))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/multiclass.py:76: UserWarning: Label 39 is present in all training examples.\n",
            "  str(classes[c]))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
            "  \"of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
            "  \"of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
            "  \"of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
            "  \"of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
            "  \"of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
            "  \"of iterations.\", ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 5 µs, sys: 3 µs, total: 8 µs\n",
            "Wall time: 9.3 µs\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1HmFQMMDSKf9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_pred_clf = clf.predict(X_test_dtm)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IXOvp6KMSKgB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "cd0b3e63-56ba-4f1f-9456-871833b7ee5e"
      },
      "source": [
        "print(metrics.accuracy_score(y_test_mlb,y_pred_clf))"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.2653333333333333\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sa4hlMtdSKgG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import classification_report"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YbmwWepfSKgJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "39ed189d-b213-433f-ebae-8e8f73b741de"
      },
      "source": [
        "print(classification_report(y_test_mlb, y_pred_clf))"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00      2250\n",
            "           1       0.71      0.19      0.30        64\n",
            "           2       0.00      0.00      0.00         0\n",
            "           3       0.86      0.46      0.60       542\n",
            "           4       0.76      0.35      0.48       526\n",
            "           5       0.80      0.83      0.81      1241\n",
            "           6       0.88      0.36      0.51       331\n",
            "           7       0.76      0.55      0.64       797\n",
            "           8       0.89      0.48      0.62       498\n",
            "           9       0.83      0.33      0.47       505\n",
            "          10       0.75      0.33      0.46         9\n",
            "          11       1.00      0.11      0.19        19\n",
            "          12       0.79      0.78      0.78      1129\n",
            "          13       0.89      0.19      0.31        43\n",
            "          14       0.83      0.16      0.27       158\n",
            "          15       0.76      0.15      0.25        87\n",
            "          16       0.95      0.54      0.69       392\n",
            "          17       1.00      0.06      0.11        33\n",
            "          18       0.67      0.27      0.38        45\n",
            "          19       0.67      0.22      0.33       186\n",
            "          20       1.00      0.04      0.08        24\n",
            "          21       0.00      0.00      0.00        14\n",
            "          22       0.97      0.31      0.47       105\n",
            "          23       1.00      0.29      0.44        28\n",
            "          24       0.83      0.49      0.62       717\n",
            "          25       0.76      0.67      0.71       823\n",
            "          26       0.79      0.40      0.53       708\n",
            "          27       1.00      0.03      0.05        40\n",
            "          28       1.00      1.00      1.00      2250\n",
            "          29       0.65      0.20      0.31       118\n",
            "          30       0.79      0.74      0.76      1141\n",
            "          31       0.85      0.62      0.72      1030\n",
            "          32       1.00      1.00      1.00      2250\n",
            "          33       0.84      0.58      0.68       896\n",
            "          34       0.78      0.60      0.68       935\n",
            "          35       0.80      0.78      0.79      1031\n",
            "          36       0.97      1.00      0.98      2172\n",
            "          37       0.79      0.43      0.55       727\n",
            "          38       1.00      1.00      1.00      2250\n",
            "          39       1.00      1.00      1.00      2250\n",
            "          40       0.99      1.00      1.00      2230\n",
            "          41       0.83      0.89      0.86      1501\n",
            "          42       0.78      0.20      0.32       288\n",
            "          43       0.78      0.21      0.33       101\n",
            "          44       0.94      0.99      0.97      2072\n",
            "          45       0.84      0.94      0.89      1681\n",
            "          46       0.79      0.49      0.60       626\n",
            "          47       0.80      0.52      0.63       763\n",
            "          48       0.89      0.17      0.28        48\n",
            "          49       0.00      0.00      0.00         5\n",
            "          50       0.79      0.56      0.66       637\n",
            "\n",
            "   micro avg       0.91      0.79      0.84     38316\n",
            "   macro avg       0.80      0.48      0.55     38316\n",
            "weighted avg       0.89      0.79      0.82     38316\n",
            " samples avg       0.90      0.79      0.84     38316\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
            "  'precision', 'predicted', average, warn_for)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1439: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
            "  'recall', 'true', average, warn_for)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SndY6DYFSKgL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "93734130-1115-412c-a569-9f437e15642b"
      },
      "source": [
        "metrics.average_precision_score(y_test_mlb, y_pred_clf,average='micro')"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7864456365321326"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NWlyEJLdSKgP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#metrics.average_precision_score(y_test_mlb, y_pred_clf,average='weighted')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_nmyw2kTSKgY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "76f28001-7004-4fe7-ea1b-1f997078e6b3"
      },
      "source": [
        "#metrics.recall_score(y_test_mlb, y_pred_clf)\n",
        "metrics.recall_score(y_test_mlb, y_pred_clf, labels=None, pos_label=1, average='micro', sample_weight=None)"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7916536172878171"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5catjwVkbrf3",
        "colab_type": "text"
      },
      "source": [
        "### 10.\t Print true label and predicted label for any five examples (7.5 points)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nph45yU0SKgm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "outputId": "caa9a3fd-16e4-43e8-f7ec-c5b9d877106d"
      },
      "source": [
        "y_pred_clf[10:15]"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0,\n",
              "        1, 1, 0, 0, 0, 0, 1],\n",
              "       [1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0,\n",
              "        1, 1, 0, 0, 0, 0, 1],\n",
              "       [1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0,\n",
              "        1, 1, 0, 1, 0, 0, 0],\n",
              "       [1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0,\n",
              "        1, 1, 0, 0, 0, 0, 0],\n",
              "       [1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0,\n",
              "        1, 1, 1, 1, 0, 0, 0]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H2DJGRhISKgp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "outputId": "6ef745cb-82c3-4d11-831b-d07983724f6f"
      },
      "source": [
        "y_test_mlb[10:15]"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0,\n",
              "        1, 1, 0, 0, 0, 0, 1],\n",
              "       [1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0,\n",
              "        1, 1, 0, 0, 0, 0, 1],\n",
              "       [1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0,\n",
              "        1, 1, 0, 1, 0, 0, 0],\n",
              "       [1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0,\n",
              "        1, 1, 0, 0, 0, 0, 1],\n",
              "       [1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1,\n",
              "        1, 1, 1, 1, 0, 0, 0]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0nW7sN4LSKgs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z9kMN8-RanIe",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    }
  ]
}