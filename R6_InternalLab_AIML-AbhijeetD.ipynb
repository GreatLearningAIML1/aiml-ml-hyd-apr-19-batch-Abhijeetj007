{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sb7Epo0VOB58"
   },
   "source": [
    "### Load tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fHpCNRv1OB5-"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import Normalizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Enable Eager Execution if you are using tensorflow 1.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable Eager Execution\n",
    "tf.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DxJDmJqqOB6K",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Collect Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FhllFLyKOB6N"
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "B4yQKMiJOB6R"
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('prices.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fgkX6SEqOB6W"
   },
   "source": [
    "### Check all columns in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7K8pWsNQOB6X"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "date       object\n",
       "symbol     object\n",
       "open      float64\n",
       "close     float64\n",
       "low       float64\n",
       "high      float64\n",
       "volume    float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['date', 'symbol', 'open', 'close', 'low', 'high', 'volume'], dtype='object')"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>symbol</th>\n",
       "      <th>open</th>\n",
       "      <th>close</th>\n",
       "      <th>low</th>\n",
       "      <th>high</th>\n",
       "      <th>volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2016-01-05 00:00:00</td>\n",
       "      <td>WLTW</td>\n",
       "      <td>123.430000</td>\n",
       "      <td>125.839996</td>\n",
       "      <td>122.309998</td>\n",
       "      <td>126.250000</td>\n",
       "      <td>2163600.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2016-01-06 00:00:00</td>\n",
       "      <td>WLTW</td>\n",
       "      <td>125.239998</td>\n",
       "      <td>119.980003</td>\n",
       "      <td>119.940002</td>\n",
       "      <td>125.540001</td>\n",
       "      <td>2386400.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2016-01-07 00:00:00</td>\n",
       "      <td>WLTW</td>\n",
       "      <td>116.379997</td>\n",
       "      <td>114.949997</td>\n",
       "      <td>114.930000</td>\n",
       "      <td>119.739998</td>\n",
       "      <td>2489500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2016-01-08 00:00:00</td>\n",
       "      <td>WLTW</td>\n",
       "      <td>115.480003</td>\n",
       "      <td>116.620003</td>\n",
       "      <td>113.500000</td>\n",
       "      <td>117.440002</td>\n",
       "      <td>2006300.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2016-01-11 00:00:00</td>\n",
       "      <td>WLTW</td>\n",
       "      <td>117.010002</td>\n",
       "      <td>114.970001</td>\n",
       "      <td>114.089996</td>\n",
       "      <td>117.330002</td>\n",
       "      <td>1408600.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  date symbol        open       close         low        high  \\\n",
       "0  2016-01-05 00:00:00   WLTW  123.430000  125.839996  122.309998  126.250000   \n",
       "1  2016-01-06 00:00:00   WLTW  125.239998  119.980003  119.940002  125.540001   \n",
       "2  2016-01-07 00:00:00   WLTW  116.379997  114.949997  114.930000  119.739998   \n",
       "3  2016-01-08 00:00:00   WLTW  115.480003  116.620003  113.500000  117.440002   \n",
       "4  2016-01-11 00:00:00   WLTW  117.010002  114.970001  114.089996  117.330002   \n",
       "\n",
       "      volume  \n",
       "0  2163600.0  \n",
       "1  2386400.0  \n",
       "2  2489500.0  \n",
       "3  2006300.0  \n",
       "4  1408600.0  "
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7dU6X7MpOB6c"
   },
   "source": [
    "### Drop columns `date` and  `symbol`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lh_6spSKOB6e"
   },
   "outputs": [],
   "source": [
    "data = data.drop(columns=['date','symbol'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xlwbUgTwOB6i",
    "outputId": "56bad82a-f271-415a-e0d6-cbe1c4290743"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>open</th>\n",
       "      <th>close</th>\n",
       "      <th>low</th>\n",
       "      <th>high</th>\n",
       "      <th>volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>123.430000</td>\n",
       "      <td>125.839996</td>\n",
       "      <td>122.309998</td>\n",
       "      <td>126.250000</td>\n",
       "      <td>2163600.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>125.239998</td>\n",
       "      <td>119.980003</td>\n",
       "      <td>119.940002</td>\n",
       "      <td>125.540001</td>\n",
       "      <td>2386400.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>116.379997</td>\n",
       "      <td>114.949997</td>\n",
       "      <td>114.930000</td>\n",
       "      <td>119.739998</td>\n",
       "      <td>2489500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>115.480003</td>\n",
       "      <td>116.620003</td>\n",
       "      <td>113.500000</td>\n",
       "      <td>117.440002</td>\n",
       "      <td>2006300.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>117.010002</td>\n",
       "      <td>114.970001</td>\n",
       "      <td>114.089996</td>\n",
       "      <td>117.330002</td>\n",
       "      <td>1408600.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         open       close         low        high     volume\n",
       "0  123.430000  125.839996  122.309998  126.250000  2163600.0\n",
       "1  125.239998  119.980003  119.940002  125.540001  2386400.0\n",
       "2  116.379997  114.949997  114.930000  119.739998  2489500.0\n",
       "3  115.480003  116.620003  113.500000  117.440002  2006300.0\n",
       "4  117.010002  114.970001  114.089996  117.330002  1408600.0"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3DBv3WWYOB6q"
   },
   "source": [
    "### Consider only first 1000 rows in the dataset for building feature set and target set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Z_hG9rGBOB6s"
   },
   "outputs": [],
   "source": [
    "data = data.head(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 5)"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>open</th>\n",
       "      <th>close</th>\n",
       "      <th>low</th>\n",
       "      <th>high</th>\n",
       "      <th>volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>521</th>\n",
       "      <td>36.380001</td>\n",
       "      <td>34.840000</td>\n",
       "      <td>34.470001</td>\n",
       "      <td>37.110001</td>\n",
       "      <td>2642400.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>455</th>\n",
       "      <td>29.150000</td>\n",
       "      <td>28.670000</td>\n",
       "      <td>28.549999</td>\n",
       "      <td>29.150000</td>\n",
       "      <td>13120900.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>110.419998</td>\n",
       "      <td>111.000000</td>\n",
       "      <td>107.300003</td>\n",
       "      <td>111.400002</td>\n",
       "      <td>563100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>741</th>\n",
       "      <td>38.700001</td>\n",
       "      <td>38.020000</td>\n",
       "      <td>37.599998</td>\n",
       "      <td>38.700001</td>\n",
       "      <td>398900.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>532</th>\n",
       "      <td>48.270000</td>\n",
       "      <td>48.310001</td>\n",
       "      <td>47.950001</td>\n",
       "      <td>48.820000</td>\n",
       "      <td>497000.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           open       close         low        high      volume\n",
       "521   36.380001   34.840000   34.470001   37.110001   2642400.0\n",
       "455   29.150000   28.670000   28.549999   29.150000  13120900.0\n",
       "14   110.419998  111.000000  107.300003  111.400002    563100.0\n",
       "741   38.700001   38.020000   37.599998   38.700001    398900.0\n",
       "532   48.270000   48.310001   47.950001   48.820000    497000.0"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert Float64 to Float32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.astype('float32')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "M3UaApqYOB6x"
   },
   "source": [
    "### Divide the data into train and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2EkKAy7fOB6y"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X1 = data.drop(columns= \"close\" , axis = 1) \n",
    "Y1 = data[\"close\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalize Train and Test Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "train_x, test_x,train_y,test_y = train_test_split(X1, Y1, test_size=.3, random_state=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "transformer = Normalizer()\n",
    "X1_train= transformer.fit_transform(train_x)\n",
    "Y1_train=np.array(train_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "v6vE4eYCOB62",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Building the graph in tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "297_qja4OB7A",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "2.Define Weights and Bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "L205qPeQOB7B"
   },
   "outputs": [],
   "source": [
    "#We are initializing weights and Bias with Zero\n",
    "w = tf.zeros(shape=(4,1))\n",
    "b = tf.zeros(shape=(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HgtWA-UIOB7F",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "3.Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JveGlx25OB7H"
   },
   "outputs": [],
   "source": [
    "def prediction(x, w, b):\n",
    "   \n",
    "   xw_matmul = tf.matmul(x, w)\n",
    "   y = tf.add(xw_matmul, b)\n",
    "   \n",
    "   return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TL1hIwf_OB7M",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "4.Loss (Cost) Function [Mean square error]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8VSWPiGXOB7P"
   },
   "outputs": [],
   "source": [
    "def loss(y_actual, y_predicted):\n",
    "   \n",
    "   diff = y_actual - y_predicted\n",
    "   sqr = tf.square(diff)\n",
    "   avg = tf.reduce_mean(sqr)\n",
    "   \n",
    "   return avg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jzG85FUlOB7U",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "5.GradientDescent Optimizer to minimize Loss [GradientDescentOptimizer]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cj802w-3OB7X"
   },
   "outputs": [],
   "source": [
    "def train(x, y_actual, w, b, learning_rate=0.01):\n",
    "   \n",
    "   #Record mathematical operations on 'tape' to calculate loss\n",
    "   with tf.GradientTape() as t:\n",
    "       \n",
    "       t.watch([w,b])\n",
    "       \n",
    "       current_prediction = prediction(x, w, b)\n",
    "       current_loss = loss(y_actual, current_prediction)\n",
    "   \n",
    "   #Calculate Gradients for Loss with respect to Weights and Bias\n",
    "   dw, db = t.gradient(current_loss,[w, b])\n",
    "   \n",
    "   #Update Weights and Bias\n",
    "   w = w - learning_rate*dw\n",
    "   b = b - learning_rate*db\n",
    "   \n",
    "   return w, b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xSypb_u8OB7e",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Execute the Graph for 100 epochs and observe the loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DVvgj7eQOB7f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Loss on iteration 0 8002.6514\n",
      "Current Loss on iteration 1 7663.568\n",
      "Current Loss on iteration 2 7351.0693\n",
      "Current Loss on iteration 3 7063.065\n",
      "Current Loss on iteration 4 6797.6504\n",
      "Current Loss on iteration 5 6553.033\n",
      "Current Loss on iteration 6 6327.599\n",
      "Current Loss on iteration 7 6119.838\n",
      "Current Loss on iteration 8 5928.3647\n",
      "Current Loss on iteration 9 5751.9062\n",
      "Current Loss on iteration 10 5589.279\n",
      "Current Loss on iteration 11 5439.4014\n",
      "Current Loss on iteration 12 5301.2764\n",
      "Current Loss on iteration 13 5173.9775\n",
      "Current Loss on iteration 14 5056.66\n",
      "Current Loss on iteration 15 4948.547\n",
      "Current Loss on iteration 16 4848.9023\n",
      "Current Loss on iteration 17 4757.0684\n",
      "Current Loss on iteration 18 4672.4365\n",
      "Current Loss on iteration 19 4594.4424\n",
      "Current Loss on iteration 20 4522.562\n",
      "Current Loss on iteration 21 4456.313\n",
      "Current Loss on iteration 22 4395.265\n",
      "Current Loss on iteration 23 4338.999\n",
      "Current Loss on iteration 24 4287.1455\n",
      "Current Loss on iteration 25 4239.3525\n",
      "Current Loss on iteration 26 4195.3125\n",
      "Current Loss on iteration 27 4154.721\n",
      "Current Loss on iteration 28 4117.3145\n",
      "Current Loss on iteration 29 4082.8394\n",
      "Current Loss on iteration 30 4051.0688\n",
      "Current Loss on iteration 31 4021.7888\n",
      "Current Loss on iteration 32 3994.8047\n",
      "Current Loss on iteration 33 3969.9346\n",
      "Current Loss on iteration 34 3947.0127\n",
      "Current Loss on iteration 35 3925.8904\n",
      "Current Loss on iteration 36 3906.4236\n",
      "Current Loss on iteration 37 3888.4844\n",
      "Current Loss on iteration 38 3871.9487\n",
      "Current Loss on iteration 39 3856.7126\n",
      "Current Loss on iteration 40 3842.6692\n",
      "Current Loss on iteration 41 3829.7275\n",
      "Current Loss on iteration 42 3817.7986\n",
      "Current Loss on iteration 43 3806.8079\n",
      "Current Loss on iteration 44 3796.6753\n",
      "Current Loss on iteration 45 3787.3428\n",
      "Current Loss on iteration 46 3778.7356\n",
      "Current Loss on iteration 47 3770.807\n",
      "Current Loss on iteration 48 3763.4978\n",
      "Current Loss on iteration 49 3756.7632\n",
      "Current Loss on iteration 50 3750.5557\n",
      "Current Loss on iteration 51 3744.8323\n",
      "Current Loss on iteration 52 3739.5645\n",
      "Current Loss on iteration 53 3734.7046\n",
      "Current Loss on iteration 54 3730.2258\n",
      "Current Loss on iteration 55 3726.0999\n",
      "Current Loss on iteration 56 3722.2969\n",
      "Current Loss on iteration 57 3718.7917\n",
      "Current Loss on iteration 58 3715.5593\n",
      "Current Loss on iteration 59 3712.584\n",
      "Current Loss on iteration 60 3709.8408\n",
      "Current Loss on iteration 61 3707.3125\n",
      "Current Loss on iteration 62 3704.9817\n",
      "Current Loss on iteration 63 3702.8342\n",
      "Current Loss on iteration 64 3700.8552\n",
      "Current Loss on iteration 65 3699.0317\n",
      "Current Loss on iteration 66 3697.3481\n",
      "Current Loss on iteration 67 3695.8005\n",
      "Current Loss on iteration 68 3694.3762\n",
      "Current Loss on iteration 69 3693.0576\n",
      "Current Loss on iteration 70 3691.846\n",
      "Current Loss on iteration 71 3690.7253\n",
      "Current Loss on iteration 72 3689.6973\n",
      "Current Loss on iteration 73 3688.75\n",
      "Current Loss on iteration 74 3687.8718\n",
      "Current Loss on iteration 75 3687.0667\n",
      "Current Loss on iteration 76 3686.3252\n",
      "Current Loss on iteration 77 3685.6409\n",
      "Current Loss on iteration 78 3685.0068\n",
      "Current Loss on iteration 79 3684.4277\n",
      "Current Loss on iteration 80 3683.8896\n",
      "Current Loss on iteration 81 3683.3992\n",
      "Current Loss on iteration 82 3682.943\n",
      "Current Loss on iteration 83 3682.5212\n",
      "Current Loss on iteration 84 3682.135\n",
      "Current Loss on iteration 85 3681.7798\n",
      "Current Loss on iteration 86 3681.4497\n",
      "Current Loss on iteration 87 3681.1477\n",
      "Current Loss on iteration 88 3680.8687\n",
      "Current Loss on iteration 89 3680.6123\n",
      "Current Loss on iteration 90 3680.3765\n",
      "Current Loss on iteration 91 3680.1562\n",
      "Current Loss on iteration 92 3679.955\n",
      "Current Loss on iteration 93 3679.7715\n",
      "Current Loss on iteration 94 3679.599\n",
      "Current Loss on iteration 95 3679.44\n",
      "Current Loss on iteration 96 3679.2974\n",
      "Current Loss on iteration 97 3679.1626\n",
      "Current Loss on iteration 98 3679.0405\n",
      "Current Loss on iteration 99 3678.9253\n"
     ]
    }
   ],
   "source": [
    "for i in range(100):\n",
    "   \n",
    "   w, b = train(X1_train, Y1_train, w, b)\n",
    "   print('Current Loss on iteration', i, loss(Y1_train, prediction(X1_train, w, b)).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9smwOW-1OB7k"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9JuLI6bSOB7n"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DOL2ncA1OB7q"
   },
   "source": [
    "### Get the shapes and values of W and b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZGvtyTeuOB7r"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights:\n",
      " [[2.5353245e-03]\n",
      " [2.5108384e-03]\n",
      " [2.5564781e-03]\n",
      " [3.3674854e+01]]\n",
      "Bias:\n",
      " [33.67486]\n"
     ]
    }
   ],
   "source": [
    "#Check Weights and Bias\n",
    "print('Weights:\\n', w.numpy())\n",
    "print('Bias:\\n',b.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vhDtOv5UOB7x"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YJRBuqXhOB7_"
   },
   "source": [
    "### Linear Classification using Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8GoNTWXAOB8C",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Building the simple Neural Network in Keras with one neuron in the dense hidden layer.\n",
    "#### Use Mean square error as loss function and sgd as optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = tf.keras.models.Sequential()\n",
    "# model.add(tf.keras.layers.BatchNormalization())\n",
    "# model.add(tf.keras.layers.Dense(1,  input_shape=(4,)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zpeL5rCTOB8D"
   },
   "outputs": [],
   "source": [
    "#Initialize Sequential Graph (model)\n",
    "model = tf.keras.Sequential()\n",
    "\n",
    "#Add Dense layer for prediction - Keras declares weights and bias automatically\n",
    "model.add(tf.keras.layers.Dense(1, input_shape=(4,)))\n",
    "\n",
    "#Compile the model - add Loss and Gradient Descent optimizer\n",
    "model.compile(optimizer='sgd', loss='mse')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Wt-HYFMEOB8G",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Execute the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "700/700 [==============================] - 0s 230us/sample - loss: nan         \n",
      "Epoch 2/100\n",
      "700/700 [==============================] - 0s 54us/sample - loss: nan\n",
      "Epoch 3/100\n",
      "700/700 [==============================] - 0s 50us/sample - loss: nan\n",
      "Epoch 4/100\n",
      "700/700 [==============================] - 0s 53us/sample - loss: nan\n",
      "Epoch 5/100\n",
      "700/700 [==============================] - 0s 63us/sample - loss: nan\n",
      "Epoch 6/100\n",
      "700/700 [==============================] - 0s 60us/sample - loss: nan\n",
      "Epoch 7/100\n",
      "700/700 [==============================] - 0s 57us/sample - loss: nan\n",
      "Epoch 8/100\n",
      "700/700 [==============================] - 0s 79us/sample - loss: nan\n",
      "Epoch 9/100\n",
      "700/700 [==============================] - 0s 56us/sample - loss: nan\n",
      "Epoch 10/100\n",
      "700/700 [==============================] - 0s 57us/sample - loss: nan\n",
      "Epoch 11/100\n",
      "700/700 [==============================] - 0s 43us/sample - loss: nan\n",
      "Epoch 12/100\n",
      "700/700 [==============================] - 0s 57us/sample - loss: nan\n",
      "Epoch 13/100\n",
      "700/700 [==============================] - 0s 66us/sample - loss: nan\n",
      "Epoch 14/100\n",
      "700/700 [==============================] - 0s 83us/sample - loss: nan\n",
      "Epoch 15/100\n",
      "700/700 [==============================] - 0s 57us/sample - loss: nan\n",
      "Epoch 16/100\n",
      "700/700 [==============================] - 0s 74us/sample - loss: nan\n",
      "Epoch 17/100\n",
      "700/700 [==============================] - 0s 57us/sample - loss: nan\n",
      "Epoch 18/100\n",
      "700/700 [==============================] - 0s 69us/sample - loss: nan\n",
      "Epoch 19/100\n",
      "700/700 [==============================] - 0s 67us/sample - loss: nan\n",
      "Epoch 20/100\n",
      "700/700 [==============================] - 0s 49us/sample - loss: nan\n",
      "Epoch 21/100\n",
      "700/700 [==============================] - 0s 56us/sample - loss: nan\n",
      "Epoch 22/100\n",
      "700/700 [==============================] - 0s 56us/sample - loss: nan\n",
      "Epoch 23/100\n",
      "700/700 [==============================] - 0s 47us/sample - loss: nan\n",
      "Epoch 24/100\n",
      "700/700 [==============================] - 0s 54us/sample - loss: nan\n",
      "Epoch 25/100\n",
      "700/700 [==============================] - 0s 70us/sample - loss: nan\n",
      "Epoch 26/100\n",
      "700/700 [==============================] - 0s 63us/sample - loss: nan\n",
      "Epoch 27/100\n",
      "700/700 [==============================] - 0s 50us/sample - loss: nan\n",
      "Epoch 28/100\n",
      "700/700 [==============================] - 0s 56us/sample - loss: nan\n",
      "Epoch 29/100\n",
      "700/700 [==============================] - 0s 60us/sample - loss: nan\n",
      "Epoch 30/100\n",
      "700/700 [==============================] - 0s 71us/sample - loss: nan\n",
      "Epoch 31/100\n",
      "700/700 [==============================] - 0s 47us/sample - loss: nan\n",
      "Epoch 32/100\n",
      "700/700 [==============================] - 0s 53us/sample - loss: nan\n",
      "Epoch 33/100\n",
      "700/700 [==============================] - 0s 70us/sample - loss: nan\n",
      "Epoch 34/100\n",
      "700/700 [==============================] - 0s 51us/sample - loss: nan\n",
      "Epoch 35/100\n",
      "700/700 [==============================] - 0s 64us/sample - loss: nan\n",
      "Epoch 36/100\n",
      "700/700 [==============================] - 0s 73us/sample - loss: nan\n",
      "Epoch 37/100\n",
      "700/700 [==============================] - 0s 103us/sample - loss: nan\n",
      "Epoch 38/100\n",
      "700/700 [==============================] - 0s 54us/sample - loss: nan\n",
      "Epoch 39/100\n",
      "700/700 [==============================] - 0s 53us/sample - loss: nan\n",
      "Epoch 40/100\n",
      "700/700 [==============================] - 0s 71us/sample - loss: nan\n",
      "Epoch 41/100\n",
      "700/700 [==============================] - 0s 54us/sample - loss: nan\n",
      "Epoch 42/100\n",
      "700/700 [==============================] - 0s 56us/sample - loss: nan\n",
      "Epoch 43/100\n",
      "700/700 [==============================] - 0s 67us/sample - loss: nan\n",
      "Epoch 44/100\n",
      "700/700 [==============================] - 0s 89us/sample - loss: nan\n",
      "Epoch 45/100\n",
      "700/700 [==============================] - 0s 61us/sample - loss: nan\n",
      "Epoch 46/100\n",
      "700/700 [==============================] - 0s 61us/sample - loss: nan\n",
      "Epoch 47/100\n",
      "700/700 [==============================] - 0s 63us/sample - loss: nan\n",
      "Epoch 48/100\n",
      "700/700 [==============================] - 0s 61us/sample - loss: nan\n",
      "Epoch 49/100\n",
      "700/700 [==============================] - 0s 51us/sample - loss: nan\n",
      "Epoch 50/100\n",
      "700/700 [==============================] - 0s 66us/sample - loss: nan\n",
      "Epoch 51/100\n",
      "700/700 [==============================] - 0s 70us/sample - loss: nan\n",
      "Epoch 52/100\n",
      "700/700 [==============================] - 0s 57us/sample - loss: nan\n",
      "Epoch 53/100\n",
      "700/700 [==============================] - 0s 56us/sample - loss: nan\n",
      "Epoch 54/100\n",
      "700/700 [==============================] - 0s 63us/sample - loss: nan\n",
      "Epoch 55/100\n",
      "700/700 [==============================] - 0s 59us/sample - loss: nan\n",
      "Epoch 56/100\n",
      "700/700 [==============================] - 0s 70us/sample - loss: nan\n",
      "Epoch 57/100\n",
      "700/700 [==============================] - 0s 61us/sample - loss: nan\n",
      "Epoch 58/100\n",
      "700/700 [==============================] - 0s 47us/sample - loss: nan\n",
      "Epoch 59/100\n",
      "700/700 [==============================] - 0s 116us/sample - loss: nan\n",
      "Epoch 60/100\n",
      "700/700 [==============================] - 0s 70us/sample - loss: nan\n",
      "Epoch 61/100\n",
      "700/700 [==============================] - 0s 63us/sample - loss: nan\n",
      "Epoch 62/100\n",
      "700/700 [==============================] - 0s 87us/sample - loss: nan\n",
      "Epoch 63/100\n",
      "700/700 [==============================] - 0s 66us/sample - loss: nan\n",
      "Epoch 64/100\n",
      "700/700 [==============================] - 0s 70us/sample - loss: nan\n",
      "Epoch 65/100\n",
      "700/700 [==============================] - 0s 63us/sample - loss: nan\n",
      "Epoch 66/100\n",
      "700/700 [==============================] - 0s 69us/sample - loss: nan\n",
      "Epoch 67/100\n",
      "700/700 [==============================] - 0s 57us/sample - loss: nan\n",
      "Epoch 68/100\n",
      "700/700 [==============================] - 0s 93us/sample - loss: nan\n",
      "Epoch 69/100\n",
      "700/700 [==============================] - 0s 64us/sample - loss: nan\n",
      "Epoch 70/100\n",
      "700/700 [==============================] - 0s 110us/sample - loss: nan\n",
      "Epoch 71/100\n",
      "700/700 [==============================] - 0s 53us/sample - loss: nan\n",
      "Epoch 72/100\n",
      "700/700 [==============================] - 0s 44us/sample - loss: nan\n",
      "Epoch 73/100\n",
      "700/700 [==============================] - 0s 56us/sample - loss: nan\n",
      "Epoch 74/100\n",
      "700/700 [==============================] - 0s 66us/sample - loss: nan\n",
      "Epoch 75/100\n",
      "700/700 [==============================] - 0s 67us/sample - loss: nan\n",
      "Epoch 76/100\n",
      "700/700 [==============================] - 0s 71us/sample - loss: nan\n",
      "Epoch 77/100\n",
      "700/700 [==============================] - 0s 54us/sample - loss: nan\n",
      "Epoch 78/100\n",
      "700/700 [==============================] - 0s 57us/sample - loss: nan\n",
      "Epoch 79/100\n",
      "700/700 [==============================] - 0s 61us/sample - loss: nan\n",
      "Epoch 80/100\n",
      "700/700 [==============================] - 0s 53us/sample - loss: nan\n",
      "Epoch 81/100\n",
      "700/700 [==============================] - 0s 63us/sample - loss: nan\n",
      "Epoch 82/100\n",
      "700/700 [==============================] - 0s 63us/sample - loss: nan\n",
      "Epoch 83/100\n",
      "700/700 [==============================] - 0s 97us/sample - loss: nan\n",
      "Epoch 84/100\n",
      "700/700 [==============================] - 0s 63us/sample - loss: nan\n",
      "Epoch 85/100\n",
      "700/700 [==============================] - 0s 86us/sample - loss: nan\n",
      "Epoch 86/100\n",
      "700/700 [==============================] - 0s 70us/sample - loss: nan\n",
      "Epoch 87/100\n",
      "700/700 [==============================] - 0s 73us/sample - loss: nan\n",
      "Epoch 88/100\n",
      "700/700 [==============================] - 0s 66us/sample - loss: nan\n",
      "Epoch 89/100\n",
      "700/700 [==============================] - 0s 57us/sample - loss: nan\n",
      "Epoch 90/100\n",
      "700/700 [==============================] - 0s 46us/sample - loss: nan\n",
      "Epoch 91/100\n",
      "700/700 [==============================] - 0s 71us/sample - loss: nan\n",
      "Epoch 92/100\n",
      "700/700 [==============================] - 0s 71us/sample - loss: nan\n",
      "Epoch 93/100\n",
      "700/700 [==============================] - 0s 79us/sample - loss: nan\n",
      "Epoch 94/100\n",
      "700/700 [==============================] - 0s 63us/sample - loss: nan\n",
      "Epoch 95/100\n",
      "700/700 [==============================] - 0s 66us/sample - loss: nan\n",
      "Epoch 96/100\n",
      "700/700 [==============================] - 0s 59us/sample - loss: nan\n",
      "Epoch 97/100\n",
      "700/700 [==============================] - 0s 66us/sample - loss: nan\n",
      "Epoch 98/100\n",
      "700/700 [==============================] - 0s 54us/sample - loss: nan\n",
      "Epoch 99/100\n",
      "700/700 [==============================] - 0s 64us/sample - loss: nan\n",
      "Epoch 100/100\n",
      "700/700 [==============================] - 0s 74us/sample - loss: nan\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x29dc4a13fd0>"
      ]
     },
     "execution_count": 248,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_x,train_y, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_9\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_23 (Dense)             (None, 1)                 5         \n",
      "=================================================================\n",
      "Total params: 5\n",
      "Trainable params: 5\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan]], dtype=float32), array([nan], dtype=float32)]"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Expected 2D array, got 1D array instead:\narray=[125.839996 119.980003 114.949997 116.620003 114.970001 115.550003\n 112.849998 114.379997 112.529999 110.379997 109.300003 110.\n 111.949997 110.120003 111.       110.709999 112.580002 114.470001\n 114.5      110.559998 114.050003 115.709999 114.019997 111.160004\n 110.650002 107.519997 107.129997 107.839996 110.769997 111.239998\n 111.599998 110.330002 113.040001 111.889999 111.559998 112.879997\n 112.75     113.32     115.510002 116.779999 117.       117.190002\n 116.949997 116.709999 116.489998 116.82     120.620003 120.629997\n 120.699997 120.82     124.029999 121.269997 121.449997 121.470001\n 119.379997 119.410004 118.720001 117.980003 118.739998 118.660004\n 119.93     117.650002 114.07     115.470001 113.540001 113.410004\n 114.639999 115.790001 117.989998 117.720001 118.449997 121.260002\n 121.68     122.309998 123.010002 123.790001 123.300003 124.400002\n 124.690002 124.150002 124.900002 125.959999 126.269997 126.559998\n 127.660004 121.559998 123.440002 125.510002 124.209999 123.889999\n 122.970001 123.910004 123.410004 125.269997 125.120003 125.57\n 125.949997 126.620003 126.459999 126.610001 127.910004 128.020004\n 128.070007 128.75     127.480003 127.68     129.25     129.029999\n 129.580002 127.010002 125.470001 124.959999 124.139999 124.839996\n 125.330002 126.269997 127.330002 126.489998 127.650002 117.010002\n 113.400002 117.559998 121.       124.309998 124.       123.480003\n 124.870003 124.440002 125.059998 125.519997 127.199997 126.419998\n 126.040001 124.589996 123.980003 124.169998 124.       123.330002\n 123.940002 124.       123.760002 124.269997 123.739998 123.620003\n 123.839996 123.169998 123.82     124.089996 119.919998 120.589996\n 121.230003 121.82     122.75     121.470001 121.529999 121.480003\n 121.93     122.559998 122.720001 122.949997 123.110001 122.209999\n 122.209999 122.989998 123.919998 123.940002 124.010002 124.849998\n 126.529999 127.660004 128.550003 125.419998 122.040001 125.25\n 123.089996 122.300003 125.120003 123.949997 124.68     125.68\n 128.399994 129.179993 128.089996 128.       128.899994 127.989998\n 129.789993 132.770004 132.110001 131.570007 131.960007 132.089996\n 131.059998 131.020004 128.729996 129.889999 127.910004 128.369995\n 127.089996 128.009995 128.759995 127.480003 126.790001 127.290001\n 126.449997 126.110001 125.629997 125.650002 125.900002 124.330002\n 122.970001 123.669998 116.779999 117.690002 118.790001 118.\n 122.269997 121.480003 122.489998 123.330002 122.669998 124.599998\n 124.82     123.889999 124.82     124.559998 125.300003 124.370003\n 124.379997 124.370003 121.419998 119.940002 121.480003 124.760002\n 124.75     123.       122.790001 121.209999 124.459999 122.260002\n 122.370003 122.330002 123.629997 123.839996 122.989998 122.849998\n 124.489998 124.279999 122.379997 122.169998 122.279999  31.300001\n   4.77      40.380001 214.009998  26.629999  54.459951  42.07\n  37.09      31.67      31.469999  42.830002  65.889999  25.67\n  27.76      34.939999  13.67      33.        47.57      40.290001\n  29.889999  30.16      22.33      25.92      38.09      34.869999\n  30.41      48.25      14.3       38.6       68.879997  57.720001\n  39.880001  43.459999 133.899994  19.27      59.75      37.939999\n 105.870003  64.739998  83.16      45.939999  16.650013  11.3\n  81.199997  18.860001  36.849998  22.65      40.919998  36.029999\n 158.029999  56.18      15.69      58.289998  39.029999  25.809999\n  40.189999  78.68      78.599998 108.        41.240002  53.639999\n  28.290001 238.580002  52.490002  25.629999   9.01      33.779999\n  67.099998   3.4       22.99      23.11      32.529999  58.549999\n  49.27      13.82      14.25      39.610001  32.060001  55.740002\n  84.269997  91.84      61.34      28.090001  59.34      36.369999\n  26.5       82.849998  61.43      30.5       16.969999 342.410011\n  87.839996  46.939999  15.79      21.700001  14.52      39.049999\n  46.23      36.310001  56.349998  37.75      52.579958  59.41\n  33.869999  74.82      24.690001  48.860001  26.389999  36.66\n  46.799999  42.84      32.98      79.059998  47.389999  38.959999\n  11.22      34.260001  56.060001  14.61      23.08      61.16\n  11.16      75.149998  32.07      31.        26.790001  50.150002\n  48.170002  83.370003  42.360001  29.18      29.1       35.\n  43.540001  16.969982  59.919998  76.57      18.15      23.9\n  45.25      45.380001  31.469999  34.77      48.540001  61.75\n  43.349998  21.110001 100.379997 109.559998  33.400002  44.790001\n  25.77      88.529999  82.620003   1.84      64.32      82.150002\n  87.470001  48.880001  35.080002  25.82      11.89      10.28\n  42.029999  83.459999  83.449997  46.77      53.98      23.82\n  49.43      10.12      11.16      32.639999  46.009998  97.009998\n  56.82      16.310002  13.929995  67.029999 135.460007  59.23\n   7.96      69.190002  15.45      11.499998  43.299999  70.900002\n  19.559999 626.751061 626.750011  38.130001  52.669998  20.540001\n  32.18     173.080002  14.79      97.239998  31.25      35.869999\n  32.540001   3.68      24.700001  44.040001  30.070001  28.67\n  63.16      23.860001  25.459999  14.94      40.349998  41.98\n  52.449999  22.66      38.279999  48.330002  53.43      11.82\n  36.220001  45.68     132.449997 112.869997  54.080002  42.009998\n  30.549999  20.879999  30.809999  27.179997   7.53      36.249995\n  22.890002 308.769989  48.57      23.34      33.060001  36.270045\n  38.450001  64.68      27.18      42.849998  37.919998  52.830002\n   5.94      13.66      36.919998  64.269992  23.98      57.040001\n  20.48      53.98      33.650002  37.18      19.309999  20.51\n  13.01      76.370003  19.690001  87.970001  30.940001  35.82\n  76.849998  25.879999  30.35      23.16      39.880001  24.75\n  11.18       1.61      17.059999 256.84      48.189999  34.84\n  27.629999  14.29      19.99      62.779999  29.18      63.360001\n  27.27      27.43      43.900002  36.299999  48.310001  44.990002\n  36.299999  91.669998  21.870001  83.019997  39.28      19.790001\n  83.43      61.709999  37.009998  32.11007   30.950001   7.969945\n  67.010002 105.639999  10.85      55.920006  18.719999  73.220001\n  20.24      53.25      48.450001  53.479999  49.919998  15.510001\n  65.349998  56.85016   46.089994  23.870001  52.59      34.189999\n  52.529999  47.790001  18.49      15.2       26.1       45.579999\n  39.619999  24.85      38.5       83.100005  30.82      16.75\n  22.92      36.610001  44.5      223.960007  28.809999  33.630001\n  61.240002  18.93      25.        61.119999  18.030001  54.52\n  10.24      20.639999  25.370001  49.349998  53.509998  33.380001\n  36.880001  60.599998  32.470001  40.349998  51.77      80.129997\n  41.099998  21.24      81.559998  50.98      46.939999  41.549999\n  53.240002  25.799999  24.629999   5.42      27.120001  31.120001\n  86.779999  81.559998  47.73      52.540001  42.689999  52.599998\n  28.68      52.209999  23.049999  37.59      19.780001  20.809999\n  22.02      61.669998  27.120001  62.18      67.110001  48.93\n  42.860001  41.91      33.25      78.919994  33.610001  24.5\n  55.16      55.900002  20.440001  44.459999  18.889999  16.120001\n  52.709999  14.82      50.830002  52.16      18.4       28.1\n  28.58      45.919998  30.870001  25.030001  15.35      48.549999\n  43.990002  36.040001  44.759992  47.5       54.400002  49.810001\n  53.34      12.24      14.81      17.209999  29.419995  26.01\n  19.040001  28.08      12.8       16.209999  31.15      18.85\n  31.530001  19.940001  65.389999  58.18      34.91      10.04\n  22.879999  71.629997  88.139999  47.169998  73.379997  29.98\n  17.889997  52.73      70.019993  29.85      24.74      44.240002\n  43.349998  33.279869  61.630001  37.299999  45.259998  49.73\n  27.32      27.85      81.949997  34.16      21.609949  54.23\n  19.09      44.279999  20.280001  63.959999  55.169998  21.08\n  18.719999  25.379999  69.150002  35.330002   8.63      17.1\n  35.090001  60.02      13.33      15.96      30.960001   5.31\n  40.139999 214.379993  26.440001  54.019953  42.330002  37.700001\n  31.620001  31.639999  42.600003  66.019997  25.280001  27.65\n  34.540001  13.53      32.529999  48.950001  39.889999  29.33\n  31.059999  22.209999  26.690001  38.02      36.779999  30.91\n  47.560001  14.19      38.43      69.43      57.220001  40.810001\n  44.18     134.690002  19.57      60.400002  37.700001 107.120003\n  66.559998  82.47      44.98      16.130013  11.32      80.589996\n  19.        37.09      22.879999  40.830002  36.330002 156.710007\n  58.02      16.200001  57.889997  38.669998  26.18      41.209999\n  79.349998  77.870003 108.839996  43.66      53.380001  28.59\n 239.610001  51.900002  25.23       9.04      35.389999  68.120003\n   3.53      22.91      23.209998  32.240002  59.25      48.16\n  14.15      14.18      39.990002  32.369999  55.09      89.25\n  93.330002  60.509998  28.970002  58.110001  35.        36.98\n  26.51      83.519997  62.099998  30.610001  16.74     342.130013\n  89.019997  48.049999  15.74      21.01      14.41      40.59\n  46.169998  36.75      56.360001  38.189999  52.629959  59.23\n  33.93      74.5       24.58      50.150002  26.35      36.720001\n  47.330002  42.869999  32.849998  79.620003  45.73      38.48\n  12.1       33.929998  55.849998  15.1       23.26      59.790001\n  11.56      75.059998  31.99      32.220001  27.969999  50.48\n  47.56      83.290001  42.160002  30.48      28.76      34.849998\n  43.049999  16.689982  59.41      76.650002  18.66      23.65\n  44.560001  44.790001  31.459999  34.459999  48.349998  60.959999\n  43.419998  20.879999  99.150002 108.540001  33.27      45.099998\n  25.68      86.919998  83.120003   1.8       63.939999  80.839996\n  88.099998  48.080002  35.32      25.93      11.81      10.96\n  42.900002  83.959999  84.540001  46.25      52.52      24.84\n  49.639999  10.39      11.47      32.720001  45.98      98.09\n  56.630001  16.239999  13.97      66.949997 137.399994  60.25\n   7.91      69.300003  15.53      12.150007  43.259998  70.239998\n  19.629999 623.991055 623.990017  38.09      52.560001  20.790001\n  32.259998 176.139999  15.75      97.279999  31.65      37.23\n  32.18       3.76      24.780001  44.66      29.990002  28.879999\n  63.549999  25.690001  25.65      14.94      40.43      44.43\n  52.669998  22.34      38.48      48.        53.279999  12.09\n  37.169998  45.66     130.850006 108.620005  54.169998  41.700001\n  30.35      20.870001  30.809999  28.140002   7.45      35.889994\n  22.959996 307.380005  48.439999  23.469999  33.150002  36.090042\n  38.959999  63.93      26.75      43.68      37.330002  52.950001\n   6.17      13.59      36.509998  64.319993  24.120001  56.349998\n  20.24      54.02      34.310001  37.380001  18.68      20.290001\n  13.55      75.440002  19.83      87.370003  30.959999  35.189999\n  77.650002  26.42      31.059999  22.92      39.610001  25.140004\n  11.77       1.65      16.860001 256.079998  48.150002  34.43\n  27.790001  14.42      19.58      62.299999  28.719999  63.59\n  26.99      28.77      44.799999  37.139999].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-222-80641be0d32b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mtransformer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mNormalizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mX1\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mY1\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mY1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[0mtrain_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_x\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtrain_y\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtest_y\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m.3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m7\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda4\\lib\\site-packages\\sklearn\\base.py\u001b[0m in \u001b[0;36mfit_transform\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    551\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    552\u001b[0m             \u001b[1;31m# fit method of arity 1 (unsupervised transformation)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 553\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    554\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    555\u001b[0m             \u001b[1;31m# fit method of arity 2 (supervised transformation)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda4\\lib\\site-packages\\sklearn\\preprocessing\\data.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m   1721\u001b[0m         \u001b[0mX\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mlike\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1722\u001b[0m         \"\"\"\n\u001b[1;32m-> 1723\u001b[1;33m         \u001b[0mcheck_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'csr'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1724\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1725\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda4\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[0;32m    519\u001b[0m                     \u001b[1;34m\"Reshape your data either using array.reshape(-1, 1) if \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    520\u001b[0m                     \u001b[1;34m\"your data has a single feature or array.reshape(1, -1) \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 521\u001b[1;33m                     \"if it contains a single sample.\".format(array))\n\u001b[0m\u001b[0;32m    522\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    523\u001b[0m         \u001b[1;31m# in the future np.flexible dtypes will be handled like object dtypes\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Expected 2D array, got 1D array instead:\narray=[125.839996 119.980003 114.949997 116.620003 114.970001 115.550003\n 112.849998 114.379997 112.529999 110.379997 109.300003 110.\n 111.949997 110.120003 111.       110.709999 112.580002 114.470001\n 114.5      110.559998 114.050003 115.709999 114.019997 111.160004\n 110.650002 107.519997 107.129997 107.839996 110.769997 111.239998\n 111.599998 110.330002 113.040001 111.889999 111.559998 112.879997\n 112.75     113.32     115.510002 116.779999 117.       117.190002\n 116.949997 116.709999 116.489998 116.82     120.620003 120.629997\n 120.699997 120.82     124.029999 121.269997 121.449997 121.470001\n 119.379997 119.410004 118.720001 117.980003 118.739998 118.660004\n 119.93     117.650002 114.07     115.470001 113.540001 113.410004\n 114.639999 115.790001 117.989998 117.720001 118.449997 121.260002\n 121.68     122.309998 123.010002 123.790001 123.300003 124.400002\n 124.690002 124.150002 124.900002 125.959999 126.269997 126.559998\n 127.660004 121.559998 123.440002 125.510002 124.209999 123.889999\n 122.970001 123.910004 123.410004 125.269997 125.120003 125.57\n 125.949997 126.620003 126.459999 126.610001 127.910004 128.020004\n 128.070007 128.75     127.480003 127.68     129.25     129.029999\n 129.580002 127.010002 125.470001 124.959999 124.139999 124.839996\n 125.330002 126.269997 127.330002 126.489998 127.650002 117.010002\n 113.400002 117.559998 121.       124.309998 124.       123.480003\n 124.870003 124.440002 125.059998 125.519997 127.199997 126.419998\n 126.040001 124.589996 123.980003 124.169998 124.       123.330002\n 123.940002 124.       123.760002 124.269997 123.739998 123.620003\n 123.839996 123.169998 123.82     124.089996 119.919998 120.589996\n 121.230003 121.82     122.75     121.470001 121.529999 121.480003\n 121.93     122.559998 122.720001 122.949997 123.110001 122.209999\n 122.209999 122.989998 123.919998 123.940002 124.010002 124.849998\n 126.529999 127.660004 128.550003 125.419998 122.040001 125.25\n 123.089996 122.300003 125.120003 123.949997 124.68     125.68\n 128.399994 129.179993 128.089996 128.       128.899994 127.989998\n 129.789993 132.770004 132.110001 131.570007 131.960007 132.089996\n 131.059998 131.020004 128.729996 129.889999 127.910004 128.369995\n 127.089996 128.009995 128.759995 127.480003 126.790001 127.290001\n 126.449997 126.110001 125.629997 125.650002 125.900002 124.330002\n 122.970001 123.669998 116.779999 117.690002 118.790001 118.\n 122.269997 121.480003 122.489998 123.330002 122.669998 124.599998\n 124.82     123.889999 124.82     124.559998 125.300003 124.370003\n 124.379997 124.370003 121.419998 119.940002 121.480003 124.760002\n 124.75     123.       122.790001 121.209999 124.459999 122.260002\n 122.370003 122.330002 123.629997 123.839996 122.989998 122.849998\n 124.489998 124.279999 122.379997 122.169998 122.279999  31.300001\n   4.77      40.380001 214.009998  26.629999  54.459951  42.07\n  37.09      31.67      31.469999  42.830002  65.889999  25.67\n  27.76      34.939999  13.67      33.        47.57      40.290001\n  29.889999  30.16      22.33      25.92      38.09      34.869999\n  30.41      48.25      14.3       38.6       68.879997  57.720001\n  39.880001  43.459999 133.899994  19.27      59.75      37.939999\n 105.870003  64.739998  83.16      45.939999  16.650013  11.3\n  81.199997  18.860001  36.849998  22.65      40.919998  36.029999\n 158.029999  56.18      15.69      58.289998  39.029999  25.809999\n  40.189999  78.68      78.599998 108.        41.240002  53.639999\n  28.290001 238.580002  52.490002  25.629999   9.01      33.779999\n  67.099998   3.4       22.99      23.11      32.529999  58.549999\n  49.27      13.82      14.25      39.610001  32.060001  55.740002\n  84.269997  91.84      61.34      28.090001  59.34      36.369999\n  26.5       82.849998  61.43      30.5       16.969999 342.410011\n  87.839996  46.939999  15.79      21.700001  14.52      39.049999\n  46.23      36.310001  56.349998  37.75      52.579958  59.41\n  33.869999  74.82      24.690001  48.860001  26.389999  36.66\n  46.799999  42.84      32.98      79.059998  47.389999  38.959999\n  11.22      34.260001  56.060001  14.61      23.08      61.16\n  11.16      75.149998  32.07      31.        26.790001  50.150002\n  48.170002  83.370003  42.360001  29.18      29.1       35.\n  43.540001  16.969982  59.919998  76.57      18.15      23.9\n  45.25      45.380001  31.469999  34.77      48.540001  61.75\n  43.349998  21.110001 100.379997 109.559998  33.400002  44.790001\n  25.77      88.529999  82.620003   1.84      64.32      82.150002\n  87.470001  48.880001  35.080002  25.82      11.89      10.28\n  42.029999  83.459999  83.449997  46.77      53.98      23.82\n  49.43      10.12      11.16      32.639999  46.009998  97.009998\n  56.82      16.310002  13.929995  67.029999 135.460007  59.23\n   7.96      69.190002  15.45      11.499998  43.299999  70.900002\n  19.559999 626.751061 626.750011  38.130001  52.669998  20.540001\n  32.18     173.080002  14.79      97.239998  31.25      35.869999\n  32.540001   3.68      24.700001  44.040001  30.070001  28.67\n  63.16      23.860001  25.459999  14.94      40.349998  41.98\n  52.449999  22.66      38.279999  48.330002  53.43      11.82\n  36.220001  45.68     132.449997 112.869997  54.080002  42.009998\n  30.549999  20.879999  30.809999  27.179997   7.53      36.249995\n  22.890002 308.769989  48.57      23.34      33.060001  36.270045\n  38.450001  64.68      27.18      42.849998  37.919998  52.830002\n   5.94      13.66      36.919998  64.269992  23.98      57.040001\n  20.48      53.98      33.650002  37.18      19.309999  20.51\n  13.01      76.370003  19.690001  87.970001  30.940001  35.82\n  76.849998  25.879999  30.35      23.16      39.880001  24.75\n  11.18       1.61      17.059999 256.84      48.189999  34.84\n  27.629999  14.29      19.99      62.779999  29.18      63.360001\n  27.27      27.43      43.900002  36.299999  48.310001  44.990002\n  36.299999  91.669998  21.870001  83.019997  39.28      19.790001\n  83.43      61.709999  37.009998  32.11007   30.950001   7.969945\n  67.010002 105.639999  10.85      55.920006  18.719999  73.220001\n  20.24      53.25      48.450001  53.479999  49.919998  15.510001\n  65.349998  56.85016   46.089994  23.870001  52.59      34.189999\n  52.529999  47.790001  18.49      15.2       26.1       45.579999\n  39.619999  24.85      38.5       83.100005  30.82      16.75\n  22.92      36.610001  44.5      223.960007  28.809999  33.630001\n  61.240002  18.93      25.        61.119999  18.030001  54.52\n  10.24      20.639999  25.370001  49.349998  53.509998  33.380001\n  36.880001  60.599998  32.470001  40.349998  51.77      80.129997\n  41.099998  21.24      81.559998  50.98      46.939999  41.549999\n  53.240002  25.799999  24.629999   5.42      27.120001  31.120001\n  86.779999  81.559998  47.73      52.540001  42.689999  52.599998\n  28.68      52.209999  23.049999  37.59      19.780001  20.809999\n  22.02      61.669998  27.120001  62.18      67.110001  48.93\n  42.860001  41.91      33.25      78.919994  33.610001  24.5\n  55.16      55.900002  20.440001  44.459999  18.889999  16.120001\n  52.709999  14.82      50.830002  52.16      18.4       28.1\n  28.58      45.919998  30.870001  25.030001  15.35      48.549999\n  43.990002  36.040001  44.759992  47.5       54.400002  49.810001\n  53.34      12.24      14.81      17.209999  29.419995  26.01\n  19.040001  28.08      12.8       16.209999  31.15      18.85\n  31.530001  19.940001  65.389999  58.18      34.91      10.04\n  22.879999  71.629997  88.139999  47.169998  73.379997  29.98\n  17.889997  52.73      70.019993  29.85      24.74      44.240002\n  43.349998  33.279869  61.630001  37.299999  45.259998  49.73\n  27.32      27.85      81.949997  34.16      21.609949  54.23\n  19.09      44.279999  20.280001  63.959999  55.169998  21.08\n  18.719999  25.379999  69.150002  35.330002   8.63      17.1\n  35.090001  60.02      13.33      15.96      30.960001   5.31\n  40.139999 214.379993  26.440001  54.019953  42.330002  37.700001\n  31.620001  31.639999  42.600003  66.019997  25.280001  27.65\n  34.540001  13.53      32.529999  48.950001  39.889999  29.33\n  31.059999  22.209999  26.690001  38.02      36.779999  30.91\n  47.560001  14.19      38.43      69.43      57.220001  40.810001\n  44.18     134.690002  19.57      60.400002  37.700001 107.120003\n  66.559998  82.47      44.98      16.130013  11.32      80.589996\n  19.        37.09      22.879999  40.830002  36.330002 156.710007\n  58.02      16.200001  57.889997  38.669998  26.18      41.209999\n  79.349998  77.870003 108.839996  43.66      53.380001  28.59\n 239.610001  51.900002  25.23       9.04      35.389999  68.120003\n   3.53      22.91      23.209998  32.240002  59.25      48.16\n  14.15      14.18      39.990002  32.369999  55.09      89.25\n  93.330002  60.509998  28.970002  58.110001  35.        36.98\n  26.51      83.519997  62.099998  30.610001  16.74     342.130013\n  89.019997  48.049999  15.74      21.01      14.41      40.59\n  46.169998  36.75      56.360001  38.189999  52.629959  59.23\n  33.93      74.5       24.58      50.150002  26.35      36.720001\n  47.330002  42.869999  32.849998  79.620003  45.73      38.48\n  12.1       33.929998  55.849998  15.1       23.26      59.790001\n  11.56      75.059998  31.99      32.220001  27.969999  50.48\n  47.56      83.290001  42.160002  30.48      28.76      34.849998\n  43.049999  16.689982  59.41      76.650002  18.66      23.65\n  44.560001  44.790001  31.459999  34.459999  48.349998  60.959999\n  43.419998  20.879999  99.150002 108.540001  33.27      45.099998\n  25.68      86.919998  83.120003   1.8       63.939999  80.839996\n  88.099998  48.080002  35.32      25.93      11.81      10.96\n  42.900002  83.959999  84.540001  46.25      52.52      24.84\n  49.639999  10.39      11.47      32.720001  45.98      98.09\n  56.630001  16.239999  13.97      66.949997 137.399994  60.25\n   7.91      69.300003  15.53      12.150007  43.259998  70.239998\n  19.629999 623.991055 623.990017  38.09      52.560001  20.790001\n  32.259998 176.139999  15.75      97.279999  31.65      37.23\n  32.18       3.76      24.780001  44.66      29.990002  28.879999\n  63.549999  25.690001  25.65      14.94      40.43      44.43\n  52.669998  22.34      38.48      48.        53.279999  12.09\n  37.169998  45.66     130.850006 108.620005  54.169998  41.700001\n  30.35      20.870001  30.809999  28.140002   7.45      35.889994\n  22.959996 307.380005  48.439999  23.469999  33.150002  36.090042\n  38.959999  63.93      26.75      43.68      37.330002  52.950001\n   6.17      13.59      36.509998  64.319993  24.120001  56.349998\n  20.24      54.02      34.310001  37.380001  18.68      20.290001\n  13.55      75.440002  19.83      87.370003  30.959999  35.189999\n  77.650002  26.42      31.059999  22.92      39.610001  25.140004\n  11.77       1.65      16.860001 256.079998  48.150002  34.43\n  27.790001  14.42      19.58      62.299999  28.719999  63.59\n  26.99      28.77      44.799999  37.139999].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample."
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "transformer = Normalizer()\n",
    "X1= transformer.fit_transform(X1)\n",
    "Y1= transformer.fit_transform(Y1)\n",
    "train_x, test_x,train_y,test_y = train_test_split(X1, Y1, test_size=.3, random_state=7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification using Keras "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the given Iris data using pandas (Iris.csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iris= mount_drive(\"Iris-1.csv\")\n",
    "iris = pd.read_csv('Iris-2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Id                 int64\n",
       "SepalLengthCm    float64\n",
       "SepalWidthCm     float64\n",
       "PetalLengthCm    float64\n",
       "PetalWidthCm     float64\n",
       "Species           object\n",
       "dtype: object"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>SepalLengthCm</th>\n",
       "      <th>SepalWidthCm</th>\n",
       "      <th>PetalLengthCm</th>\n",
       "      <th>PetalWidthCm</th>\n",
       "      <th>Species</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id  SepalLengthCm  SepalWidthCm  PetalLengthCm  PetalWidthCm      Species\n",
       "0   1            5.1           3.5            1.4           0.2  Iris-setosa\n",
       "1   2            4.9           3.0            1.4           0.2  Iris-setosa\n",
       "2   3            4.7           3.2            1.3           0.2  Iris-setosa"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris[\"Species\"] = pd.Categorical(iris[\"Species\"]).codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris.drop(\"Id\",axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting the data into feature set and target set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = iris.drop(columns= ['Species']) \n",
    "\n",
    "target = iris[['Species']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SepalLengthCm</th>\n",
       "      <th>SepalWidthCm</th>\n",
       "      <th>PetalLengthCm</th>\n",
       "      <th>PetalWidthCm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   SepalLengthCm  SepalWidthCm  PetalLengthCm  PetalWidthCm\n",
       "0            5.1           3.5            1.4           0.2\n",
       "1            4.9           3.0            1.4           0.2\n",
       "2            4.7           3.2            1.3           0.2\n",
       "3            4.6           3.1            1.5           0.2\n",
       "4            5.0           3.6            1.4           0.2"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "features.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Species</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Species\n",
       "0        0\n",
       "1        0\n",
       "2        0\n",
       "3        0\n",
       "4        0"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Target set has different categories. So, Label encode them. And convert into one-hot vectors using get_dummies in pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_processed = pd.DataFrame(target).apply(LabelEncoder().fit_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "testY = tf.keras.utils.to_categorical(Y_processed, num_classes=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.]], dtype=float32)"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Divide the dataset into Training and test (70:30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "transformer = Normalizer()\n",
    "#features = transformer.fit_transform(features)\n",
    "\n",
    "train_x, test_x,train_y,test_y = train_test_split(features, testY, test_size=.3, random_state=7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model\n",
    "Build the model with following layers: <br>\n",
    "1. First dense layer with 10 neurons with input shape 4 (according to the feature set) <br>\n",
    "2. Second Dense layer with 8 neurons <br>\n",
    "3. Output layer with 3 neurons with softmax activation (output layer, 3 neurons as we have 3 classes) <br>\n",
    "4. Use SGD and categorical_crossentropy loss "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential()\n",
    "model.add(tf.keras.layers.BatchNormalization())\n",
    "\n",
    "model.add(tf.keras.layers.Dense(10, activation='relu', input_shape=(4,)))\n",
    "model.add(tf.keras.layers.Dense(8, activation='relu'))\n",
    "model.add(tf.keras.layers.Dense(3, activation='softmax'))\n",
    "\n",
    "#Create optimizer with non-default learning rate\n",
    "sgd_optimizer = tf.keras.optimizers.SGD(lr=0.03)\n",
    "\n",
    "#Compile the model\n",
    "model.compile(optimizer=sgd_optimizer, loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting the model and predicting "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 84 samples, validate on 21 samples\n",
      "Epoch 1/30\n",
      "84/84 [==============================] - 1s 14ms/sample - loss: 1.0736 - acc: 0.2976 - val_loss: 1.2921 - val_acc: 0.3810\n",
      "Epoch 2/30\n",
      "84/84 [==============================] - 0s 262us/sample - loss: 1.0281 - acc: 0.2976 - val_loss: 1.1440 - val_acc: 0.3810\n",
      "Epoch 3/30\n",
      "84/84 [==============================] - 0s 262us/sample - loss: 0.9982 - acc: 0.2976 - val_loss: 1.0671 - val_acc: 0.3810\n",
      "Epoch 4/30\n",
      "84/84 [==============================] - 0s 297us/sample - loss: 0.9725 - acc: 0.3333 - val_loss: 1.0171 - val_acc: 0.3810\n",
      "Epoch 5/30\n",
      "84/84 [==============================] - 0s 202us/sample - loss: 0.9646 - acc: 0.4762 - val_loss: 0.9582 - val_acc: 0.3810\n",
      "Epoch 6/30\n",
      "84/84 [==============================] - 0s 309us/sample - loss: 0.9379 - acc: 0.5476 - val_loss: 0.9221 - val_acc: 0.3810\n",
      "Epoch 7/30\n",
      "84/84 [==============================] - 0s 202us/sample - loss: 0.9164 - acc: 0.6667 - val_loss: 0.8937 - val_acc: 0.3810\n",
      "Epoch 8/30\n",
      "84/84 [==============================] - 0s 238us/sample - loss: 0.8981 - acc: 0.7143 - val_loss: 0.8676 - val_acc: 0.3810\n",
      "Epoch 9/30\n",
      "84/84 [==============================] - 0s 214us/sample - loss: 0.8807 - acc: 0.7381 - val_loss: 0.8519 - val_acc: 0.6667\n",
      "Epoch 10/30\n",
      "84/84 [==============================] - 0s 321us/sample - loss: 0.8660 - acc: 0.7619 - val_loss: 0.8370 - val_acc: 0.7619\n",
      "Epoch 11/30\n",
      "84/84 [==============================] - 0s 297us/sample - loss: 0.8580 - acc: 0.7262 - val_loss: 0.8252 - val_acc: 0.8095\n",
      "Epoch 12/30\n",
      "84/84 [==============================] - 0s 405us/sample - loss: 0.8303 - acc: 0.7381 - val_loss: 0.8136 - val_acc: 0.8571\n",
      "Epoch 13/30\n",
      "84/84 [==============================] - 0s 167us/sample - loss: 0.8289 - acc: 0.7976 - val_loss: 0.8028 - val_acc: 0.9048\n",
      "Epoch 14/30\n",
      "84/84 [==============================] - 0s 381us/sample - loss: 0.8079 - acc: 0.7500 - val_loss: 0.7928 - val_acc: 0.9048\n",
      "Epoch 15/30\n",
      "84/84 [==============================] - 0s 226us/sample - loss: 0.8013 - acc: 0.7738 - val_loss: 0.7859 - val_acc: 0.9048\n",
      "Epoch 16/30\n",
      "84/84 [==============================] - 0s 274us/sample - loss: 0.7776 - acc: 0.7500 - val_loss: 0.7763 - val_acc: 0.9048\n",
      "Epoch 17/30\n",
      "84/84 [==============================] - 0s 286us/sample - loss: 0.7468 - acc: 0.7738 - val_loss: 0.7675 - val_acc: 0.9048\n",
      "Epoch 18/30\n",
      "84/84 [==============================] - 0s 369us/sample - loss: 0.7276 - acc: 0.7976 - val_loss: 0.7602 - val_acc: 0.8571\n",
      "Epoch 19/30\n",
      "84/84 [==============================] - 0s 274us/sample - loss: 0.7281 - acc: 0.7738 - val_loss: 0.7513 - val_acc: 0.9048\n",
      "Epoch 20/30\n",
      "84/84 [==============================] - 0s 262us/sample - loss: 0.7085 - acc: 0.7738 - val_loss: 0.7444 - val_acc: 0.8095\n",
      "Epoch 21/30\n",
      "84/84 [==============================] - 0s 250us/sample - loss: 0.6946 - acc: 0.7619 - val_loss: 0.7393 - val_acc: 0.8095\n",
      "Epoch 22/30\n",
      "84/84 [==============================] - 0s 238us/sample - loss: 0.6752 - acc: 0.7619 - val_loss: 0.7333 - val_acc: 0.8095\n",
      "Epoch 23/30\n",
      "84/84 [==============================] - 0s 190us/sample - loss: 0.6649 - acc: 0.7976 - val_loss: 0.7286 - val_acc: 0.7619\n",
      "Epoch 24/30\n",
      "84/84 [==============================] - 0s 190us/sample - loss: 0.6528 - acc: 0.8095 - val_loss: 0.7240 - val_acc: 0.6667\n",
      "Epoch 25/30\n",
      "84/84 [==============================] - 0s 155us/sample - loss: 0.6363 - acc: 0.8214 - val_loss: 0.7180 - val_acc: 0.6667\n",
      "Epoch 26/30\n",
      "84/84 [==============================] - 0s 202us/sample - loss: 0.6256 - acc: 0.7857 - val_loss: 0.7143 - val_acc: 0.6190\n",
      "Epoch 27/30\n",
      "84/84 [==============================] - 0s 250us/sample - loss: 0.6115 - acc: 0.7857 - val_loss: 0.7091 - val_acc: 0.6190\n",
      "Epoch 28/30\n",
      "84/84 [==============================] - 0s 416us/sample - loss: 0.5890 - acc: 0.7976 - val_loss: 0.7017 - val_acc: 0.6190\n",
      "Epoch 29/30\n",
      "84/84 [==============================] - 0s 274us/sample - loss: 0.5845 - acc: 0.8095 - val_loss: 0.6975 - val_acc: 0.6190\n",
      "Epoch 30/30\n",
      "84/84 [==============================] - 0s 226us/sample - loss: 0.5749 - acc: 0.8333 - val_loss: 0.6911 - val_acc: 0.6190\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x29dbad34c88>"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#train model\n",
    "model.fit(train_x.values, train_y, validation_split=0.2, epochs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Report Accuracy of the predicted values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45/45 [==============================] - 0s 8ms/sample - loss: 0.7459 - acc: 0.6444\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.7459042800797356, 0.64444447]"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(test_x,test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "batch_normalization_7 (Batch multiple                  16        \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             multiple                  50        \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             multiple                  88        \n",
      "_________________________________________________________________\n",
      "dense_21 (Dense)             multiple                  27        \n",
      "=================================================================\n",
      "Total params: 181\n",
      "Trainable params: 173\n",
      "Non-trainable params: 8\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Linear+classification+using+Tensorflow+and+Keras.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
